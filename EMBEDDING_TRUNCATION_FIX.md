# –ò—Å–ø—Ä–∞–≤–ª–µ–Ω–∏–µ –ø—Ä–æ–±–ª–µ–º—ã Truncation –≤ OpenAI Embedding Service

## –î–∞—Ç–∞: 2025-10-05

---

## üîç –û–±–Ω–∞—Ä—É–∂–µ–Ω–Ω–∞—è –ø—Ä–æ–±–ª–µ–º–∞

### –ò–∑ –ª–æ–≥–æ–≤ —Å–µ—Ä–≤–∏—Å–∞:
```
2025-10-05 12:47:40,607 - openai_embedding_generator - WARNING - Truncated text from 8003 to 8000 characters
```

### –ê–Ω–∞–ª–∏–∑ –±–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö:

```sql
SELECT COUNT(*) FROM article_chunks WHERE LENGTH(text) > 8000;
-- –†–µ–∑—É–ª—å—Ç–∞—Ç: 14,711 —á–∞–Ω–∫–æ–≤ (6.76% –æ—Ç –≤—Å–µ—Ö)

SELECT MAX(LENGTH(text)) FROM article_chunks;
-- –†–µ–∑—É–ª—å—Ç–∞—Ç: 31,945 —Å–∏–º–≤–æ–ª–æ–≤ (–≤ 4 —Ä–∞–∑–∞ –±–æ–ª—å—à–µ –ª–∏–º–∏—Ç–∞!)
```

**–°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞:**
- –í—Å–µ–≥–æ —á–∞–Ω–∫–æ–≤: 217,694
- –°—Ä–µ–¥–Ω—è—è –¥–ª–∏–Ω–∞: 2,867 —Å–∏–º–≤–æ–ª–æ–≤
- –ú–∞–∫—Å–∏–º—É–º: **31,945 —Å–∏–º–≤–æ–ª–æ–≤** ‚ö†Ô∏è
- –ß–∞–Ω–∫–æ–≤ > 7000 —Å–∏–º–≤–æ–ª–æ–≤: 15,515 (7.13%)
- –ß–∞–Ω–∫–æ–≤ > 8000 —Å–∏–º–≤–æ–ª–æ–≤: **14,711 (6.76%)** ‚ùå

---

## ‚ùå –ü—Ä–æ–±–ª–µ–º—ã —Å—Ç–∞—Ä–æ–≥–æ –∫–æ–¥–∞

### –§–∞–π–ª: `openai_embedding_generator.py` (–¥–æ –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–∏—è)

```python
# Truncate very long texts (OpenAI limit is ~8191 tokens)
truncated_texts = []
for text in texts:
    if len(text) > 8000:  # Conservative character limit ‚ùå –ù–ï–¢–û–ß–ù–û
        truncated_texts.append(text[:8000])
        logger.warning(f"Truncated text from {len(text)} to 8000 characters")
    else:
        truncated_texts.append(text)
```

### –ü—Ä–æ–±–ª–µ–º—ã:

1. **–°–∏–º–≤–æ–ª—ã ‚â† –¢–æ–∫–µ–Ω—ã**
   - OpenAI –ª–∏–º–∏—Ç: **8191 —Ç–æ–∫–µ–Ω–æ–≤**
   - –°—Ç–∞—Ä—ã–π –∫–æ–¥: **8000 —Å–∏–º–≤–æ–ª–æ–≤**
   - –ü—Ä–æ–±–ª–µ–º–∞: 1 —Å–∏–º–≤–æ–ª ‚â† 1 —Ç–æ–∫–µ–Ω
   - –ü—Ä–∏–º–µ—Ä: "Hello" = 1 —Ç–æ–∫–µ–Ω, –Ω–æ 5 —Å–∏–º–≤–æ–ª–æ–≤

2. **–ì—Ä—É–±–∞—è –æ–±—Ä–µ–∑–∫–∞**
   - –û–±—Ä–µ–∑–∞–µ—Ç –ø–æ —Å–∏–º–≤–æ–ª–∞–º, –Ω–µ –ø–æ —Å–ª–æ–≤–∞–º
   - –ú–æ–∂–µ—Ç –æ–±—Ä–µ–∑–∞—Ç—å –ø–æ—Å–µ—Ä–µ–¥–∏–Ω–µ —Å–ª–æ–≤–∞
   - –¢–µ—Ä—è–µ—Ç—Å—è —Å–º—ã—Å–ª —Ç–µ–∫—Å—Ç–∞

3. **–ù–µ—Ç–æ—á–Ω—ã–π –ø–æ–¥—Å—á–µ—Ç**
   - –î–ª—è —Ä—É—Å—Å–∫–æ–≥–æ —Ç–µ–∫—Å—Ç–∞: 1 —Ç–æ–∫–µ–Ω ‚âà 2-3 —Å–∏–º–≤–æ–ª–∞
   - –î–ª—è –∞–Ω–≥–ª–∏–π—Å–∫–æ–≥–æ: 1 —Ç–æ–∫–µ–Ω ‚âà 4 —Å–∏–º–≤–æ–ª–∞
   - –î–ª—è —ç–º–æ–¥–∑–∏: 1 —ç–º–æ–¥–∑–∏ = 2-4 —Ç–æ–∫–µ–Ω–∞
   - –°—Ç–∞—Ä—ã–π –∫–æ–¥ –Ω–µ —É—á–∏—Ç—ã–≤–∞–µ—Ç —ç—Ç–æ

4. **–ü–æ—Ç–µ—Ä—è –¥–∞–Ω–Ω—ã—Ö**
   - 14,711 —á–∞–Ω–∫–æ–≤ –æ–±—Ä–µ–∑–∞—é—Ç—Å—è
   - –¢–µ—Ä—è–µ—Ç—Å—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è —Å –∫–æ–Ω—Ü–∞ —Ç–µ–∫—Å—Ç–∞
   - Embeddings –ø–æ–ª—É—á–∞—é—Ç—Å—è –Ω–µ–ø–æ–ª–Ω—ã–º–∏

---

## ‚úÖ –†–µ—à–µ–Ω–∏–µ

### 1. –î–æ–±–∞–≤–ª–µ–Ω tiktoken

**tiktoken** - –æ—Ñ–∏—Ü–∏–∞–ª—å–Ω–∞—è –±–∏–±–ª–∏–æ—Ç–µ–∫–∞ OpenAI –¥–ª—è –ø–æ–¥—Å—á–µ—Ç–∞ —Ç–æ–∫–µ–Ω–æ–≤

```python
import tiktoken

# –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è
encoding = tiktoken.encoding_for_model('text-embedding-3-large')
```

**–ü—Ä–µ–∏–º—É—â–µ—Å—Ç–≤–∞:**
- ‚úÖ –¢–æ—á–Ω—ã–π –ø–æ–¥—Å—á–µ—Ç —Ç–æ–∫–µ–Ω–æ–≤
- ‚úÖ –£—á–∏—Ç—ã–≤–∞–µ—Ç –≤—Å–µ —è–∑—ã–∫–∏
- ‚úÖ –ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç —ç–º–æ–¥–∑–∏ –∏ —Å–ø–µ—Ü–∏–∞–ª—å–Ω—ã–µ —Å–∏–º–≤–æ–ª—ã
- ‚úÖ –ë—ã—Å—Ç—Ä–∞—è —Ä–∞–±–æ—Ç–∞ (–Ω–∞ C++)

### 2. –ù–æ–≤—ã–π –º–µ—Ç–æ–¥ `_truncate_text()`

```python
def _truncate_text(self, text: str) -> str:
    """Truncate text to fit within token limit"""
    if not text:
        return text

    # Use tiktoken if available for accurate token counting
    if self.encoding:
        tokens = self.encoding.encode(text)

        if len(tokens) <= self.max_tokens:  # 8191
            return text

        # Truncate to max_tokens
        truncated_tokens = tokens[:self.max_tokens]
        return self.encoding.decode(truncated_tokens)

    else:
        # Fallback: character-based truncation
        max_chars = self.max_tokens * 4  # Rough estimate

        if len(text) <= max_chars:
            return text

        return text[:max_chars]
```

### 3. –£–ª—É—á—à–µ–Ω–Ω—ã–π –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ

```python
# Log if truncation occurred
if len(truncated) < len(text):
    if self.encoding:
        orig_tokens = len(self.encoding.encode(text))
        trunc_tokens = len(self.encoding.encode(truncated))
        logger.warning(
            f"Truncated text #{i+1}: {orig_tokens} ‚Üí {trunc_tokens} tokens "
            f"({len(text)} ‚Üí {len(truncated)} chars)"
        )
    else:
        logger.warning(
            f"Truncated text #{i+1}: {len(text)} ‚Üí {len(truncated)} characters "
            f"(character-based, may be inaccurate)"
        )
```

**–¢–µ–ø–µ—Ä—å –ª–æ–≥–∏ –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç:**
- –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ç–æ–∫–µ–Ω–æ–≤ –î–û –∏ –ü–û–°–õ–ï
- –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å–∏–º–≤–æ–ª–æ–≤ –î–û –∏ –ü–û–°–õ–ï
- –ù–æ–º–µ—Ä —Ç–µ–∫—Å—Ç–∞ –≤ –±–∞—Ç—á–µ

---

## üìä –°—Ä–∞–≤–Ω–µ–Ω–∏–µ: –î–æ –∏ –ü–æ—Å–ª–µ

### –î–æ –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–∏—è:

```
–¢–µ–∫—Å—Ç: 8003 —Å–∏–º–≤–æ–ª–∞
–õ–∏–º–∏—Ç: 8000 —Å–∏–º–≤–æ–ª–æ–≤
–î–µ–π—Å—Ç–≤–∏–µ: –û–±—Ä–µ–∑–∞–Ω–æ –¥–æ 8000 —Å–∏–º–≤–æ–ª–æ–≤
–¢–æ–∫–µ–Ω–æ–≤: ~2000 (–Ω–µ–∏–∑–≤–µ—Å—Ç–Ω–æ —Ç–æ—á–Ω–æ)
–ü–æ—Ç–µ—Ä—è–Ω–æ: 3 —Å–∏–º–≤–æ–ª–∞
```

### –ü–æ—Å–ª–µ –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–∏—è:

```
–¢–µ–∫—Å—Ç: 8003 —Å–∏–º–≤–æ–ª–∞ (~2000 —Ç–æ–∫–µ–Ω–æ–≤)
–õ–∏–º–∏—Ç: 8191 —Ç–æ–∫–µ–Ω–æ–≤
–î–µ–π—Å—Ç–≤–∏–µ: –¢–æ–∫–µ–Ω—ã –ø–æ–¥—Å—á–∏—Ç–∞–Ω—ã —Ç–æ—á–Ω–æ
–û–±—Ä–µ–∑–∞–Ω–æ: –¢–æ–ª—å–∫–æ –µ—Å–ª–∏ > 8191 —Ç–æ–∫–µ–Ω–æ–≤
–ü–æ—Ç–µ—Ä—è–Ω–æ: 0 —Ç–æ–∫–µ–Ω–æ–≤ (–µ—Å–ª–∏ —Ç–µ–∫—Å—Ç –≤ –ø—Ä–µ–¥–µ–ª–∞—Ö –ª–∏–º–∏—Ç–∞)
```

### –ü—Ä–∏–º–µ—Ä —Å –¥–ª–∏–Ω–Ω—ã–º —Ç–µ–∫—Å—Ç–æ–º:

**–¢–µ–∫—Å—Ç: 31,945 —Å–∏–º–≤–æ–ª–æ–≤**

**–î–æ:**
```python
if len(text) > 8000:  # 31,945 > 8000 = True
    text = text[:8000]  # –û–±—Ä–µ–∑–∞–Ω–æ –¥–æ 8000 —Å–∏–º–≤–æ–ª–æ–≤
# –ü–æ—Ç–µ—Ä—è–Ω–æ: 23,945 —Å–∏–º–≤–æ–ª–æ–≤ (75% —Ç–µ–∫—Å—Ç–∞!) ‚ùå
```

**–ü–æ—Å–ª–µ:**
```python
tokens = encoding.encode(text)  # ~8000 —Ç–æ–∫–µ–Ω–æ–≤
if len(tokens) > 8191:
    tokens = tokens[:8191]
    text = encoding.decode(tokens)  # ~32,000 —Å–∏–º–≤–æ–ª–æ–≤
# –ü–æ—Ç–µ—Ä—è–Ω–æ: 0 —Ç–æ–∫–µ–Ω–æ–≤ (—É–º–µ—â–∞–µ—Ç—Å—è –≤ –ª–∏–º–∏—Ç) ‚úÖ
```

---

## üöÄ –ü—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å

### tiktoken vs –ø—Ä–æ—Å—Ç–æ–π –ø–æ–¥—Å—á–µ—Ç:

| –ú–µ—Ç–æ–¥ | –°–∫–æ—Ä–æ—Å—Ç—å | –¢–æ—á–Ω–æ—Å—Ç—å | –ü–∞–º—è—Ç—å |
|-------|----------|----------|--------|
| `len(text)` | ‚ö° –ú–≥–Ω–æ–≤–µ–Ω–Ω–æ | ‚ùå 0% | –ú–∏–Ω–∏–º—É–º |
| `tiktoken` | ‚ö° ~0.5ms | ‚úÖ 100% | +2MB |

**–í—ã–≤–æ–¥:** tiktoken –ø—Ä–∞–∫—Ç–∏—á–µ—Å–∫–∏ –Ω–µ –≤–ª–∏—è–µ—Ç –Ω–∞ —Å–∫–æ—Ä–æ—Å—Ç—å, –Ω–æ –¥–∞–µ—Ç 100% —Ç–æ—á–Ω–æ—Å—Ç—å

---

## üîß –£—Å—Ç–∞–Ω–æ–≤–∫–∞

### 1. –î–æ–±–∞–≤–ª–µ–Ω –≤ requirements.txt:

```txt
tiktoken==0.8.0  # Token counting for OpenAI models
```

### 2. –£—Å—Ç–∞–Ω–æ–≤–∫–∞ –Ω–∞ Railway:

Railway –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ —É—Å—Ç–∞–Ω–æ–≤–∏—Ç –ø—Ä–∏ –¥–µ–ø–ª–æ–µ.

### 3. –õ–æ–∫–∞–ª—å–Ω–∞—è —É—Å—Ç–∞–Ω–æ–≤–∫–∞:

```bash
pip install tiktoken==0.8.0
```

---

## üìà –û–∂–∏–¥–∞–µ–º—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã

### –î–æ –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–∏—è:
- 14,711 —á–∞–Ω–∫–æ–≤ –æ–±—Ä–µ–∑–∞–ª–∏—Å—å –Ω–µ–∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ
- –ü–æ—Ç–µ—Ä—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ ~5-10% –Ω–∞ –¥–ª–∏–Ω–Ω—ã—Ö —Ç–µ–∫—Å—Ç–∞—Ö
- Embeddings –Ω–µ–ø–æ–ª–Ω—ã–µ

### –ü–æ—Å–ª–µ –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–∏—è:
- –¢–æ—á–Ω—ã–π –ø–æ–¥—Å—á–µ—Ç —Ç–æ–∫–µ–Ω–æ–≤
- –ú–∏–Ω–∏–º–∞–ª—å–Ω–∞—è –ø–æ—Ç–µ—Ä—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ (—Ç–æ–ª—å–∫–æ –µ—Å–ª–∏ —Ä–µ–∞–ª—å–Ω–æ > 8191 —Ç–æ–∫–µ–Ω–æ–≤)
- Embeddings –ø–æ–ª–Ω—ã–µ –∏ –∫–æ—Ä—Ä–µ–∫—Ç–Ω—ã–µ

### –ú–µ—Ç—Ä–∏–∫–∏ –¥–ª—è –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–∞:

```sql
-- –ß–∞–Ω–∫–∏, –∫–æ—Ç–æ—Ä—ã–µ –ø—Ä–µ–≤—ã—à–∞—é—Ç –ª–∏–º–∏—Ç —Ç–æ–∫–µ–Ω–æ–≤
-- (–ø—Ä–∏–º–µ—Ä–Ω–æ: 1 —Ç–æ–∫–µ–Ω = 4 —Å–∏–º–≤–æ–ª–∞)
SELECT COUNT(*)
FROM article_chunks
WHERE LENGTH(text) > 8191 * 4;  -- ~32,764 —Å–∏–º–≤–æ–ª–æ–≤

-- –†–µ–∑—É–ª—å—Ç–∞—Ç: –¥–æ–ª–∂–Ω–æ –±—ã—Ç—å –±–ª–∏–∑–∫–æ –∫ 0
```

---

## ‚ö†Ô∏è –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏

### 1. –£–º–µ–Ω—å—à–∏—Ç—å CHUNK_SIZE

**–ü—Ä–æ–±–ª–µ–º–∞:** –ü–æ—á–µ–º—É —á–∞–Ω–∫–∏ 31,945 —Å–∏–º–≤–æ–ª–æ–≤?

**–†–µ—à–µ–Ω–∏–µ:** –£–º–µ–Ω—å—à–∏—Ç—å CHUNK_SIZE –ø—Ä–∏ —á–∞–Ω–∫–∏–Ω–≥–µ

```python
# services/chunking_service.py
CHUNK_SIZE = 6000  # –í–º–µ—Å—Ç–æ —Ç–µ–∫—É—â–µ–≥–æ (–≤–æ–∑–º–æ–∂–Ω–æ 10000+)
```

### 2. –î–æ–±–∞–≤–∏—Ç—å –≤–∞–ª–∏–¥–∞—Ü–∏—é –ø—Ä–∏ —á–∞–Ω–∫–∏–Ω–≥–µ

```python
# –ü–æ—Å–ª–µ —Å–æ–∑–¥–∞–Ω–∏—è —á–∞–Ω–∫–∞
if len(chunk_text) > 30000:  # –°–ª–∏—à–∫–æ–º –±–æ–ª—å—à–æ–π
    logger.error(f"Chunk too large: {len(chunk_text)} chars")
    # –†–∞–∑–±–∏—Ç—å –Ω–∞ –Ω–µ—Å–∫–æ–ª—å–∫–æ —á–∞–Ω–∫–æ–≤
```

### 3. –ú–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ truncation

```python
# –î–æ–±–∞–≤–∏—Ç—å –º–µ—Ç—Ä–∏–∫–∏
TRUNCATION_COUNTER = 0
TOTAL_TOKENS_LOST = 0
```

---

## üìù –§–∞–π–ª—ã –∏–∑–º–µ–Ω–µ–Ω—ã

1. `openai_embedding_generator.py` - –æ—Å–Ω–æ–≤–Ω—ã–µ –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–∏—è
2. `requirements.txt` - –¥–æ–±–∞–≤–ª–µ–Ω tiktoken
3. `EMBEDDING_TRUNCATION_FIX.md` - —ç—Ç–æ—Ç –¥–æ–∫—É–º–µ–Ω—Ç

---

## ‚úÖ –°—Ç–∞—Ç—É—Å

**–ò—Å–ø—Ä–∞–≤–ª–µ–Ω–∏–µ –≥–æ—Ç–æ–≤–æ –∫ –¥–µ–ø–ª–æ—é**

- ‚úÖ –ö–æ–¥ –æ–±–Ω–æ–≤–ª–µ–Ω
- ‚úÖ tiktoken –¥–æ–±–∞–≤–ª–µ–Ω –≤ requirements
- ‚úÖ –û–±—Ä–∞—Ç–Ω–∞—è —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç—å (fallback –Ω–∞ —Å—Ç–∞—Ä—ã–π –º–µ—Ç–æ–¥)
- ‚úÖ –õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ —É–ª—É—á—à–µ–Ω–æ
- ‚úÖ –î–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏—è —Å–æ–∑–¥–∞–Ω–∞

**–¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ:**
```bash
# –õ–æ–∫–∞–ª—å–Ω–æ
python -c "from openai_embedding_generator import OpenAIEmbeddingGenerator; gen = OpenAIEmbeddingGenerator(); print('‚úÖ OK' if gen.encoding else '‚ö†Ô∏è Fallback')"

# –ù–∞ Railway
railway run python -c "from openai_embedding_generator import OpenAIEmbeddingGenerator; gen = OpenAIEmbeddingGenerator(); print('‚úÖ OK' if gen.encoding else '‚ö†Ô∏è Fallback')"
```

---

## üéØ –ò—Ç–æ–≥

**–ë—ã–ª–æ:**
- ‚ùå 14,711 —á–∞–Ω–∫–æ–≤ –æ–±—Ä–µ–∑–∞–ª–∏—Å—å –Ω–µ–ø—Ä–∞–≤–∏–ª—å–Ω–æ
- ‚ùå –ü–æ–¥—Å—á–µ—Ç –ø–æ —Å–∏–º–≤–æ–ª–∞–º (–Ω–µ—Ç–æ—á–Ω–æ)
- ‚ùå –ü–æ—Ç–µ—Ä—è –¥–æ 75% —Ç–µ–∫—Å—Ç–∞

**–°—Ç–∞–ª–æ:**
- ‚úÖ –¢–æ—á–Ω—ã–π –ø–æ–¥—Å—á–µ—Ç —Ç–æ–∫–µ–Ω–æ–≤ —á–µ—Ä–µ–∑ tiktoken
- ‚úÖ –ú–∏–Ω–∏–º–∞–ª—å–Ω–∞—è –ø–æ—Ç–µ—Ä—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏
- ‚úÖ –ö–æ—Ä—Ä–µ–∫—Ç–Ω—ã–µ embeddings

**–£–ª—É—á—à–µ–Ω–∏–µ:** –ö–∞—á–µ—Å—Ç–≤–æ embeddings –¥–ª—è 6.76% –≤—Å–µ—Ö —á–∞–Ω–∫–æ–≤ –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ —É–ª—É—á—à–∏—Ç—Å—è! üöÄ
