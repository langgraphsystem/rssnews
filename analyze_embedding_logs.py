#!/usr/bin/env python3
"""
–ê–Ω–∞–ª–∏–∑ –ª–æ–≥–æ–≤ OpenAI Embedding Service
–ü—Ä–æ–≤–µ—Ä—è–µ—Ç –ø—Ä–æ–±–ª–µ–º—ã —Å truncation –∏ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å
"""
import re
from datetime import datetime
from collections import defaultdict

# –õ–æ–≥–∏ –∏–∑ —Å–µ—Ä–≤–∏—Å–∞
logs = """
__main__ - INFO - Migrating embeddings for 12 chunks
2025-10-05 12:47:40,607 - openai_embedding_generator - WARNING - Truncated text from 8003 to 8000 characters
2025-10-05 12:47:41,441 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings "HTTP/1.1 200 OK"
2025-10-05 12:47:41,449 - openai_embedding_generator - INFO - Embedding generation complete: 12 successful / 12 total
2025-10-05 12:47:41,590 - __main__ - INFO - Migration progress: 12/12 successful (0 errors)
2025-10-05 12:47:41,590 - __main__ - INFO - Migration complete: 12 successful, 0 errors
"""

print("="*100)
print("–ê–ù–ê–õ–ò–ó –õ–û–ì–û–í OpenAI EMBEDDING SERVICE")
print("="*100)

# –ü–∞—Ä—Å–∏–º –ª–æ–≥–∏
lines = [l.strip() for l in logs.strip().split('\n') if l.strip()]

print(f"\nüìä –û–±—â–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è:")
print(f"   –í—Å–µ–≥–æ –∑–∞–ø–∏—Å–µ–π: {len(lines)}")

# –ò–∑–≤–ª–µ–∫–∞–µ–º –∫–ª—é—á–µ–≤—ã–µ –¥–∞–Ω–Ω—ã–µ
batch_size = None
truncations = []
success_count = None
error_count = None
timing = []

for line in lines:
    # Batch size
    if 'Migrating embeddings for' in line:
        match = re.search(r'for (\d+) chunks', line)
        if match:
            batch_size = int(match.group(1))

    # Truncation warnings
    if 'Truncated text from' in line:
        match = re.search(r'from (\d+) to (\d+)', line)
        if match:
            truncations.append({
                'from': int(match.group(1)),
                'to': int(match.group(2)),
                'lost': int(match.group(1)) - int(match.group(2))
            })

    # Success/error counts
    if 'successful / ' in line:
        match = re.search(r'(\d+) successful / (\d+) total', line)
        if match:
            success_count = int(match.group(1))
            total_count = int(match.group(2))

    # Timing
    if re.match(r'\d{4}-\d{2}-\d{2} \d{2}:\d{2}:\d{2}', line):
        timestamp = line.split(' - ')[0]
        timing.append(timestamp)

# –ê–Ω–∞–ª–∏–∑ –±–∞—Ç—á–∞
print(f"\nüì¶ –ë–ê–¢–ß –û–ë–†–ê–ë–û–¢–ö–ò:")
print(f"   –†–∞–∑–º–µ—Ä –±–∞—Ç—á–∞: {batch_size} chunks")
print(f"   –£—Å–ø–µ—à–Ω–æ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–æ: {success_count}/{batch_size}")
print(f"   –û—à–∏–±–æ–∫: 0")
print(f"   –£—Å–ø–µ—à–Ω–æ—Å—Ç—å: {success_count/batch_size*100:.1f}%")

# –ê–Ω–∞–ª–∏–∑ truncation
print(f"\n‚ö†Ô∏è  –ü–†–û–ë–õ–ï–ú–ê: TRUNCATION (–û–±—Ä–µ–∑–∫–∞ —Ç–µ–∫—Å—Ç–∞)")
print(f"   –ù–∞–π–¥–µ–Ω–æ truncation: {len(truncations)}")

if truncations:
    for i, trunc in enumerate(truncations, 1):
        print(f"\n   Truncation #{i}:")
        print(f"      –ò—Å—Ö–æ–¥–Ω–∞—è –¥–ª–∏–Ω–∞: {trunc['from']:,} —Å–∏–º–≤–æ–ª–æ–≤")
        print(f"      –û–±—Ä–µ–∑–∞–Ω–æ –¥–æ:    {trunc['to']:,} —Å–∏–º–≤–æ–ª–æ–≤")
        print(f"      –ü–æ—Ç–µ—Ä—è–Ω–æ:       {trunc['lost']:,} —Å–∏–º–≤–æ–ª–æ–≤ ({trunc['lost']/trunc['from']*100:.1f}%)")

    print(f"\n   ‚ùå –ö–†–ò–¢–ò–ß–ï–°–ö–ê–Ø –ü–†–û–ë–õ–ï–ú–ê:")
    print(f"      OpenAI API –∏–º–µ–µ—Ç –ª–∏–º–∏—Ç 8191 —Ç–æ–∫–µ–Ω–æ–≤ –Ω–∞ –∑–∞–ø—Ä–æ—Å")
    print(f"      –¢–µ–∫—É—â–∏–π –ª–∏–º–∏—Ç –≤ –∫–æ–¥–µ: 8000 —Å–∏–º–≤–æ–ª–æ–≤ (–æ—á–µ–Ω—å –ø—Ä–∏–±–ª–∏–∑–∏—Ç–µ–ª—å–Ω–æ)")
    print(f"      –°—Ç–∞—Ç—å—è —Å–æ–¥–µ—Ä–∂–∞–ª–∞ 8003 —Å–∏–º–≤–æ–ª–∞ - –±—ã–ª–∞ –æ–±—Ä–µ–∑–∞–Ω–∞")

# –ê–Ω–∞–ª–∏–∑ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏
if len(timing) >= 2:
    try:
        start_time = datetime.strptime(timing[0], '%Y-%m-%d %H:%M:%S,%f')
        end_time = datetime.strptime(timing[-1], '%Y-%m-%d %H:%M:%S,%f')
        duration = (end_time - start_time).total_seconds()

        print(f"\n‚è±Ô∏è  –ü–†–û–ò–ó–í–û–î–ò–¢–ï–õ–¨–ù–û–°–¢–¨:")
        print(f"   –í—Ä–µ–º—è –æ–±—Ä–∞–±–æ—Ç–∫–∏: {duration:.2f} —Å–µ–∫—É–Ω–¥")
        print(f"   –°–∫–æ—Ä–æ—Å—Ç—å: {batch_size/duration:.1f} chunks/sec")
        print(f"   –í—Ä–µ–º—è –Ω–∞ 1 chunk: {duration/batch_size:.2f} sec")
    except:
        pass

# –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏
print(f"\n{'='*100}")
print("–†–ï–ö–û–ú–ï–ù–î–ê–¶–ò–ò –ü–û –ò–°–ü–†–ê–í–õ–ï–ù–ò–Æ")
print(f"{'='*100}")

print(f"\n1Ô∏è‚É£  –ü–†–û–ë–õ–ï–ú–ê TRUNCATION")
print(f"   –§–∞–π–ª: services/openai_embedding_generator.py")
print(f"   –¢–µ–∫—É—â–∏–π –∫–æ–¥:")
print(f"   ```python")
print(f"   MAX_CHARS = 8000  # –°–ª–∏—à–∫–æ–º –≥—Ä—É–±–æ–µ –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–µ")
print(f"   if len(text) > MAX_CHARS:")
print(f"       text = text[:MAX_CHARS]")
print(f"   ```")

print(f"\n   ‚úÖ –†–ï–®–ï–ù–ò–ï 1: –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å tiktoken –¥–ª—è —Ç–æ—á–Ω–æ–≥–æ –ø–æ–¥—Å—á–µ—Ç–∞ —Ç–æ–∫–µ–Ω–æ–≤")
print(f"   ```python")
print(f"   import tiktoken")
print(f"   ")
print(f"   encoding = tiktoken.encoding_for_model('text-embedding-3-large')")
print(f"   tokens = encoding.encode(text)")
print(f"   ")
print(f"   MAX_TOKENS = 8191  # OpenAI limit")
print(f"   if len(tokens) > MAX_TOKENS:")
print(f"       tokens = tokens[:MAX_TOKENS]")
print(f"       text = encoding.decode(tokens)")
print(f"   ```")

print(f"\n   ‚úÖ –†–ï–®–ï–ù–ò–ï 2: –†–∞–∑–±–∏–≤–∞—Ç—å –¥–ª–∏–Ω–Ω—ã–µ —Ç–µ–∫—Å—Ç—ã –Ω–∞ –Ω–µ—Å–∫–æ–ª—å–∫–æ —á–∞–Ω–∫–æ–≤")
print(f"   - –ï—Å–ª–∏ —Ç–µ–∫—Å—Ç > 8191 —Ç–æ–∫–µ–Ω–æ–≤, —Å–æ–∑–¥–∞—Ç—å 2+ embedding")
print(f"   - –£—Å—Ä–µ–¥–Ω–∏—Ç—å embeddings –∏–ª–∏ —Ö—Ä–∞–Ω–∏—Ç—å –æ—Ç–¥–µ–ª—å–Ω–æ")

print(f"\n2Ô∏è‚É£  –ü–†–û–ë–õ–ï–ú–ê: –ü–æ—á–µ–º—É —á–∞–Ω–∫ 8003 —Å–∏–º–≤–æ–ª–∞?")
print(f"   –§–∞–π–ª: services/chunking_service.py")
print(f"   ")
print(f"   –í–æ–∑–º–æ–∂–Ω–∞—è –ø—Ä–∏—á–∏–Ω–∞:")
print(f"   - CHUNK_SIZE —Å–ª–∏—à–∫–æ–º –±–æ–ª—å—à–æ–π")
print(f"   - –ù–µ—Ç –≤–∞–ª–∏–¥–∞—Ü–∏–∏ –¥–ª–∏–Ω—ã –ø—Ä–∏ —Å–æ–∑–¥–∞–Ω–∏–∏ —á–∞–Ω–∫–∞")
print(f"   ")
print(f"   ‚úÖ –†–ï–®–ï–ù–ò–ï:")
print(f"   - –£—Å—Ç–∞–Ω–æ–≤–∏—Ç—å CHUNK_SIZE=6000 —Å–∏–º–≤–æ–ª–æ–≤")
print(f"   - –î–æ–±–∞–≤–∏—Ç—å –ø—Ä–æ–≤–µ—Ä–∫—É –ø–µ—Ä–µ–¥ —Å–æ–∑–¥–∞–Ω–∏–µ–º —á–∞–Ω–∫–∞")
print(f"   - –†–∞–∑–±–∏–≤–∞—Ç—å –¥–ª–∏–Ω–Ω—ã–µ –ø–∞—Ä–∞–≥—Ä–∞—Ñ—ã")

print(f"\n3Ô∏è‚É£  –ú–û–ù–ò–¢–û–†–ò–ù–ì")
print(f"   –î–æ–±–∞–≤–∏—Ç—å –º–µ—Ç—Ä–∏–∫–∏:")
print(f"   - –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ truncation –∑–∞ –¥–µ–Ω—å")
print(f"   - –°—Ä–µ–¥–Ω—è—è –¥–ª–∏–Ω–∞ –æ–±—Ä–µ–∑–∞–Ω–Ω–æ–≥–æ —Ç–µ–∫—Å—Ç–∞")
print(f"   - –ü—Ä–æ—Ü–µ–Ω—Ç –ø–æ—Ç–µ—Ä—è–Ω–Ω–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏")

print(f"\n4Ô∏è‚É£  –ü–†–û–í–ï–†–ö–ê –î–ê–ù–ù–´–•")
print(f"   –ù–∞–π—Ç–∏ –≤—Å–µ —á–∞–Ω–∫–∏ —Å –ø—Ä–æ–±–ª–µ–º–Ω–æ–π –¥–ª–∏–Ω–æ–π:")

print(f"\n{'='*100}")
print("SQL –ó–ê–ü–†–û–°–´ –î–õ–Ø –ü–†–û–í–ï–†–ö–ò")
print(f"{'='*100}")

print(f"\n-- –ù–∞–π—Ç–∏ –≤—Å–µ –¥–ª–∏–Ω–Ω—ã–µ —á–∞–Ω–∫–∏")
print(f"SELECT id, LENGTH(text) as text_length, title_norm")
print(f"FROM article_chunks")
print(f"WHERE LENGTH(text) > 7000")
print(f"ORDER BY LENGTH(text) DESC")
print(f"LIMIT 20;")

print(f"\n-- –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –¥–ª–∏–Ω—ã —á–∞–Ω–∫–æ–≤")
print(f"SELECT")
print(f"  COUNT(*) as total,")
print(f"  AVG(LENGTH(text)) as avg_length,")
print(f"  MAX(LENGTH(text)) as max_length,")
print(f"  MIN(LENGTH(text)) as min_length,")
print(f"  COUNT(*) FILTER (WHERE LENGTH(text) > 7000) as over_7000,")
print(f"  COUNT(*) FILTER (WHERE LENGTH(text) > 8000) as over_8000")
print(f"FROM article_chunks;")

print(f"\n-- –ù–∞–π—Ç–∏ —á–∞–Ω–∫–∏, –∫–æ—Ç–æ—Ä—ã–µ –±—ã–ª–∏ –æ–±—Ä–µ–∑–∞–Ω—ã (–µ—Å–ª–∏ –µ—Å—Ç—å –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ)")
print(f"SELECT id, title_norm, LENGTH(text)")
print(f"FROM article_chunks")
print(f"WHERE metadata->>'truncated' = 'true'")
print(f"   OR LENGTH(text) = 8000  -- –†–æ–≤–Ω–æ 8000 = –≤–µ—Ä–æ—è—Ç–Ω–æ –æ–±—Ä–µ–∑–∞–Ω–æ")
print(f"LIMIT 10;")

print(f"\n{'='*100}")
print("–ò–¢–û–ì")
print(f"{'='*100}")

print(f"\n‚úÖ –°–µ—Ä–≤–∏—Å —Ä–∞–±–æ—Ç–∞–µ—Ç:")
print(f"   - 12/12 chunks —É—Å–ø–µ—à–Ω–æ –æ–±—Ä–∞–±–æ—Ç–∞–Ω—ã")
print(f"   - Embeddings —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã")
print(f"   - –ù–µ—Ç –∫—Ä–∏—Ç–∏—á–µ—Å–∫–∏—Ö –æ—à–∏–±–æ–∫")

print(f"\n‚ö†Ô∏è  –ï—Å—Ç—å –ø—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏–µ:")
print(f"   - 1 —Ç–µ–∫—Å—Ç –æ–±—Ä–µ–∑–∞–Ω —Å 8003 –¥–æ 8000 —Å–∏–º–≤–æ–ª–æ–≤")
print(f"   - –ü–æ—Ç–µ—Ä—è–Ω–æ 3 —Å–∏–º–≤–æ–ª–∞ (0.04%)")
print(f"   - –ù–µ –∫—Ä–∏—Ç–∏—á–Ω–æ –¥–ª—è –æ–¥–Ω–æ–≥–æ —Å–ª—É—á–∞—è")

print(f"\n‚ùå –ü–æ—Ç–µ–Ω—Ü–∏–∞–ª—å–Ω—ã–µ –ø—Ä–æ–±–ª–µ–º—ã:")
print(f"   - –ï—Å–ª–∏ –º–Ω–æ–≥–æ —Ç–µ–∫—Å—Ç–æ–≤ > 8000 —Å–∏–º–≤–æ–ª–æ–≤, –±—É–¥–µ—Ç –º–Ω–æ–≥–æ truncation")
print(f"   - –¢–µ—Ä—è–µ—Ç—Å—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –∏–∑ –∫–æ–Ω—Ü–∞ —Ç–µ–∫—Å—Ç–∞")
print(f"   - Embeddings –º–æ–≥—É—Ç –±—ã—Ç—å –Ω–µ–ø–æ–ª–Ω—ã–º–∏")

print(f"\nüîß –ù—É–∂–Ω–æ –∏—Å–ø—Ä–∞–≤–∏—Ç—å:")
print(f"   1. –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å tiktoken –≤–º–µ—Å—Ç–æ –ø—Ä–æ—Å—Ç–æ–≥–æ –ø–æ–¥—Å—á–µ—Ç–∞ —Å–∏–º–≤–æ–ª–æ–≤")
print(f"   2. –£–º–µ–Ω—å—à–∏—Ç—å CHUNK_SIZE –¥–æ 6000-7000 —Å–∏–º–≤–æ–ª–æ–≤")
print(f"   3. –î–æ–±–∞–≤–∏—Ç—å –≤–∞–ª–∏–¥–∞—Ü–∏—é –ø—Ä–∏ —á–∞–Ω–∫–∏–Ω–≥–µ")
print(f"   4. –ú–æ–Ω–∏—Ç–æ—Ä–∏—Ç—å truncation –º–µ—Ç—Ä–∏–∫–∏")
