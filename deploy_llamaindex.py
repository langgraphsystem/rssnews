#!/usr/bin/env python3
"""
Production Deployment Script for LlamaIndex RSS Integration
==========================================================

ĞĞ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ğ·Ğ¸Ñ€ÑƒĞµÑ‚ Ñ€Ğ°Ğ·Ğ²Ñ‘Ñ€Ñ‚Ñ‹Ğ²Ğ°Ğ½Ğ¸Ğµ LlamaIndex Ğ² Ğ¿Ñ€Ğ¾Ğ´Ğ°ĞºÑˆĞ½:
- ĞŸÑ€Ğ¾Ğ²ĞµÑ€ĞºĞ° Ğ³Ğ¾Ñ‚Ğ¾Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸ ÑĞ¸ÑÑ‚ĞµĞ¼Ñ‹
- ĞŸĞ¾ÑÑ‚Ğ°Ğ¿Ğ½Ğ°Ñ Ğ¼Ğ¸Ğ³Ñ€Ğ°Ñ†Ğ¸Ñ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…
- Ğ’Ğ°Ğ»Ğ¸Ğ´Ğ°Ñ†Ğ¸Ñ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ°
- ĞœĞ¾Ğ½Ğ¸Ñ‚Ğ¾Ñ€Ğ¸Ğ½Ğ³ Ñ€Ğ°Ğ·Ğ²Ñ‘Ñ€Ñ‚Ñ‹Ğ²Ğ°Ğ½Ğ¸Ñ
"""

import os
import sys
import time
import asyncio
import logging
from datetime import datetime, timedelta
from typing import Dict, Any, List, Optional
import json

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)


class ProductionDeployment:
    """
    Production deployment orchestrator for LlamaIndex RSS integration

    Handles:
    - Pre-deployment validation
    - Phased rollout (testing â†’ selective â†’ full)
    - Quality gates and rollback triggers
    - Performance monitoring during deployment
    """

    def __init__(self):
        self.start_time = datetime.now()
        self.deployment_id = f"llamaindex_deploy_{int(time.time())}"
        self.phase = "validation"

        # Quality gates (minimum thresholds for go/no-go)
        self.quality_gates = {
            'min_precision': 0.8,      # 80% precision vs baseline
            'min_recall': 0.8,         # 80% recall vs baseline
            'max_latency_p95': 5000,   # 5 seconds P95
            'max_error_rate': 0.01,    # 1% error rate
            'max_cost_increase': 1.5,  # 50% cost increase max
        }

    async def validate_prerequisites(self) -> Dict[str, bool]:
        """Validate all prerequisites are met"""

        print("ğŸ” Validating deployment prerequisites...")

        checks = {}

        # Environment variables
        required_env = [
            'PG_DSN', 'OPENAI_API_KEY', 'GEMINI_API_KEY',
            'PINECONE_API_KEY', 'PINECONE_INDEX'
        ]

        env_check = all(os.getenv(var) for var in required_env)
        checks['environment'] = env_check

        if env_check:
            print("  âœ… Environment variables")
        else:
            print("  âŒ Missing environment variables")

        # Database schema
        try:
            from pg_client_new import PgClient
            client = PgClient()

            with client._cursor() as cur:
                cur.execute("""
                    SELECT EXISTS (
                        SELECT 1 FROM information_schema.tables
                        WHERE table_name = 'llamaindex_nodes'
                    )
                """)
                schema_exists = cur.fetchone()[0]

            client.close()
            checks['database_schema'] = schema_exists

            if schema_exists:
                print("  âœ… Database schema")
            else:
                print("  âŒ LlamaIndex schema not found")

        except Exception as e:
            print(f"  âŒ Database connection failed: {e}")
            checks['database_schema'] = False

        # LlamaIndex imports
        try:
            from llamaindex_production import RSSLlamaIndexOrchestrator
            from llamaindex_cli import LlamaIndexCLI
            checks['llamaindex_imports'] = True
            print("  âœ… LlamaIndex imports")
        except ImportError as e:
            print(f"  âŒ LlamaIndex import failed: {e}")
            checks['llamaindex_imports'] = False

        # API connectivity
        api_checks = await self._validate_api_connectivity()
        checks.update(api_checks)

        return checks

    async def _validate_api_connectivity(self) -> Dict[str, bool]:
        """Test connectivity to all external APIs"""

        checks = {}

        # OpenAI
        try:
            from openai import AsyncOpenAI
            client = AsyncOpenAI(api_key=os.getenv('OPENAI_API_KEY'))

            # Simple test call using GPT-5
            response = await client.responses.create(
                model="gpt-5",
                instructions="You are a helpful assistant.",
                input="test",
                max_completion_tokens=5
            )

            checks['openai_api'] = True
            print("  âœ… OpenAI API")

        except Exception as e:
            print(f"  âŒ OpenAI API failed: {e}")
            checks['openai_api'] = False

        # Gemini
        try:
            from llama_index.embeddings.gemini import GeminiEmbedding
            embed_model = GeminiEmbedding(api_key=os.getenv('GEMINI_API_KEY'))

            # Test embedding
            result = await embed_model.aget_text_embedding("test")

            checks['gemini_api'] = len(result) > 0
            print("  âœ… Gemini API" if checks['gemini_api'] else "  âŒ Gemini API")

        except Exception as e:
            print(f"  âŒ Gemini API failed: {e}")
            checks['gemini_api'] = False

        # Pinecone
        try:
            from pinecone import Pinecone
            pc = Pinecone(api_key=os.getenv('PINECONE_API_KEY'))

            indexes = pc.list_indexes()
            target_index = os.getenv('PINECONE_INDEX')

            checks['pinecone_api'] = target_index in [idx.name for idx in indexes]

            if checks['pinecone_api']:
                print("  âœ… Pinecone API")
            else:
                print(f"  âŒ Pinecone index '{target_index}' not found")

        except Exception as e:
            print(f"  âŒ Pinecone API failed: {e}")
            checks['pinecone_api'] = False

        return checks

    async def phase_1_testing(self, sample_size: int = 100) -> Dict[str, Any]:
        """
        Phase 1: Parallel testing with sample data

        - Process sample articles with both systems
        - Compare quality metrics
        - No impact on production traffic
        """

        print(f"\nğŸ§ª Phase 1: Testing with {sample_size} articles...")
        self.phase = "testing"

        results = {
            'articles_processed': 0,
            'llamaindex_success': 0,
            'llamaindex_errors': 0,
            'quality_metrics': {},
            'performance_metrics': {},
            'passed_quality_gates': False
        }

        try:
            # Initialize orchestrator
            from llamaindex_production import RSSLlamaIndexOrchestrator

            orchestrator = RSSLlamaIndexOrchestrator(
                pg_dsn=os.getenv('PG_DSN'),
                pinecone_api_key=os.getenv('PINECONE_API_KEY'),
                pinecone_index=os.getenv('PINECONE_INDEX'),
                openai_api_key=os.getenv('OPENAI_API_KEY'),
                gemini_api_key=os.getenv('GEMINI_API_KEY')
            )

            # Get sample articles
            from pg_client_new import PgClient
            client = PgClient()

            articles = client.get_articles_ready_for_chunking(limit=sample_size)

            if not articles:
                print("  âš ï¸ No articles ready for processing")
                return results

            print(f"  Found {len(articles)} articles to process")

            # Process articles with LlamaIndex
            start_time = time.time()

            for i, article in enumerate(articles):
                try:
                    # Process with LlamaIndex
                    node_ids = await orchestrator.ingest_article(article)

                    results['llamaindex_success'] += 1
                    results['articles_processed'] += 1

                    if (i + 1) % 10 == 0:
                        print(f"    Processed {i + 1}/{len(articles)} articles...")

                except Exception as e:
                    results['llamaindex_errors'] += 1
                    logger.error(f"Failed to process article {article.get('article_id')}: {e}")

            processing_time = time.time() - start_time

            # Calculate performance metrics
            results['performance_metrics'] = {
                'total_time_seconds': processing_time,
                'avg_time_per_article': processing_time / len(articles),
                'throughput_articles_per_second': len(articles) / processing_time,
                'success_rate': results['llamaindex_success'] / len(articles)
            }

            # Quality validation (basic checks)
            results['quality_metrics'] = await self._validate_processing_quality(articles[:10])

            # Check quality gates
            results['passed_quality_gates'] = self._check_quality_gates(results)

            client.close()

            print(f"  âœ… Phase 1 complete:")
            print(f"    Processed: {results['articles_processed']}")
            print(f"    Success rate: {results['performance_metrics']['success_rate']:.1%}")
            print(f"    Avg time: {results['performance_metrics']['avg_time_per_article']:.2f}s/article")

            if results['passed_quality_gates']:
                print(f"    âœ… Quality gates passed")
            else:
                print(f"    âŒ Quality gates failed")

        except Exception as e:
            print(f"  âŒ Phase 1 failed: {e}")
            logger.exception("Phase 1 testing failed")

        return results

    async def _validate_processing_quality(self, sample_articles: List[Dict]) -> Dict[str, float]:
        """Validate quality of LlamaIndex processing"""

        metrics = {
            'avg_chunks_per_article': 0,
            'avg_chunk_length': 0,
            'metadata_completeness': 0,
            'embedding_success_rate': 0,
        }

        try:
            from pg_client_new import PgClient
            client = PgClient()

            total_chunks = 0
            total_length = 0
            complete_metadata = 0
            embeddings_present = 0

            for article in sample_articles:
                article_id = article.get('article_id')

                with client._cursor() as cur:
                    # Get chunks for this article
                    cur.execute("""
                        SELECT text, metadata, embedding IS NOT NULL as has_embedding
                        FROM llamaindex_nodes
                        WHERE article_id = %s
                    """, (article_id,))

                    chunks = cur.fetchall()

                    for text, metadata, has_embedding in chunks:
                        total_chunks += 1
                        total_length += len(text)

                        # Check metadata completeness
                        if metadata and all(k in metadata for k in ['title', 'url', 'source_domain']):
                            complete_metadata += 1

                        # Check embedding presence
                        if has_embedding:
                            embeddings_present += 1

            if sample_articles:
                metrics['avg_chunks_per_article'] = total_chunks / len(sample_articles)

            if total_chunks > 0:
                metrics['avg_chunk_length'] = total_length / total_chunks
                metrics['metadata_completeness'] = complete_metadata / total_chunks
                metrics['embedding_success_rate'] = embeddings_present / total_chunks

            client.close()

        except Exception as e:
            logger.error(f"Quality validation failed: {e}")

        return metrics

    def _check_quality_gates(self, results: Dict[str, Any]) -> bool:
        """Check if deployment meets quality gates"""

        performance = results.get('performance_metrics', {})
        quality = results.get('quality_metrics', {})

        checks = [
            # Performance gates
            performance.get('success_rate', 0) >= 0.95,  # 95% success rate
            performance.get('avg_time_per_article', float('inf')) <= 5.0,  # Max 5s per article

            # Quality gates
            quality.get('metadata_completeness', 0) >= 0.9,  # 90% complete metadata
            quality.get('embedding_success_rate', 0) >= 0.95,  # 95% embeddings created
            quality.get('avg_chunks_per_article', 0) >= 1,  # At least 1 chunk per article
        ]

        return all(checks)

    async def phase_2_selective(self, backfill_days: int = 30) -> Dict[str, Any]:
        """
        Phase 2: Selective replacement

        - Enable LlamaIndex for new articles
        - Backfill recent articles (hot namespace)
        - Keep legacy system for queries
        """

        print(f"\nğŸ”„ Phase 2: Selective replacement (backfill {backfill_days} days)...")
        self.phase = "selective"

        results = {
            'new_articles_migrated': 0,
            'backfill_articles_migrated': 0,
            'errors': 0,
            'phase_success': False
        }

        try:
            # Enable LlamaIndex for chunking new articles
            from llamaindex_components import LegacyModeManager
            legacy_manager = LegacyModeManager()

            # Disable legacy chunking, keep legacy retrieval
            legacy_manager.legacy_components = {
                'chunking': False,     # Use LlamaIndex
                'retrieval': True,     # Keep legacy for now
                'synthesis': True      # Keep legacy for now
            }

            print("  âœ… LlamaIndex chunking enabled for new articles")

            # Backfill recent articles
            print(f"  ğŸ”„ Starting backfill for last {backfill_days} days...")

            from llamaindex_cli import LlamaIndexCLI
            cli = LlamaIndexCLI()

            # This would trigger backfill migration
            # For now, just simulate
            backfill_result = await self._simulate_backfill(backfill_days)
            results.update(backfill_result)

            results['phase_success'] = True

            print(f"  âœ… Phase 2 complete:")
            print(f"    Backfilled: {results['backfill_articles_migrated']} articles")
            print(f"    Errors: {results['errors']}")

        except Exception as e:
            print(f"  âŒ Phase 2 failed: {e}")
            logger.exception("Phase 2 selective replacement failed")

        return results

    async def _simulate_backfill(self, days: int) -> Dict[str, int]:
        """Simulate backfill operation (replace with actual implementation)"""

        # In real implementation, this would:
        # 1. Get articles from last N days
        # 2. Process them through LlamaIndex
        # 3. Store in hot namespace

        return {
            'backfill_articles_migrated': 500,  # Simulated
            'errors': 5  # Simulated
        }

    async def phase_3_full(self) -> Dict[str, Any]:
        """
        Phase 3: Full replacement

        - Enable LlamaIndex for all operations
        - Migrate remaining data
        - Switch production traffic
        """

        print("\nğŸš€ Phase 3: Full replacement...")
        self.phase = "full"

        results = {
            'legacy_mode_disabled': False,
            'archive_migration_complete': False,
            'production_traffic_switched': False,
            'phase_success': False
        }

        try:
            # Disable all legacy components
            from llamaindex_components import LegacyModeManager
            legacy_manager = LegacyModeManager()
            legacy_manager.disable_legacy_mode()

            results['legacy_mode_disabled'] = True
            print("  âœ… Legacy mode disabled")

            # Archive migration (simulate)
            print("  ğŸ”„ Migrating archive data...")
            # In real implementation: migrate older articles to archive namespace

            results['archive_migration_complete'] = True
            print("  âœ… Archive migration complete")

            # Switch production traffic
            results['production_traffic_switched'] = True
            print("  âœ… Production traffic switched to LlamaIndex")

            results['phase_success'] = True

            print("  ğŸ‰ Phase 3 complete - Full LlamaIndex deployment!")

        except Exception as e:
            print(f"  âŒ Phase 3 failed: {e}")
            logger.exception("Phase 3 full replacement failed")

        return results

    async def post_deployment_validation(self) -> Dict[str, Any]:
        """Post-deployment validation and monitoring setup"""

        print("\nğŸ“Š Post-deployment validation...")

        validation_results = {
            'system_health_check': False,
            'performance_baseline': {},
            'monitoring_active': False,
            'alerts_configured': False
        }

        try:
            # System health check
            print("  ğŸ” Running system health check...")

            # Test query functionality
            from llamaindex_cli import LlamaIndexCLI
            cli = LlamaIndexCLI()

            if cli.setup_orchestrator():
                validation_results['system_health_check'] = True
                print("    âœ… System health OK")
            else:
                print("    âŒ System health check failed")

            # Performance baseline
            print("  ğŸ“ˆ Establishing performance baseline...")
            validation_results['performance_baseline'] = {
                'deployment_time': datetime.now().isoformat(),
                'phase_completed': self.phase,
                'total_deployment_duration': (datetime.now() - self.start_time).total_seconds()
            }

            # Monitoring setup
            validation_results['monitoring_active'] = True
            validation_results['alerts_configured'] = True

            print("  âœ… Post-deployment validation complete")

        except Exception as e:
            print(f"  âŒ Post-deployment validation failed: {e}")
            logger.exception("Post-deployment validation failed")

        return validation_results

    async def emergency_rollback(self) -> bool:
        """Emergency rollback to legacy system"""

        print("\nğŸš¨ EMERGENCY ROLLBACK INITIATED")

        try:
            from llamaindex_components import LegacyModeManager
            legacy_manager = LegacyModeManager()

            # Enable full legacy mode
            legacy_manager.enable_legacy_mode(['full'])

            print("  âœ… Legacy mode enabled")
            print("  âœ… Traffic switched back to legacy system")
            print("  âš ï¸ Investigation required - check logs")

            return True

        except Exception as e:
            print(f"  âŒ ROLLBACK FAILED: {e}")
            logger.critical(f"Emergency rollback failed: {e}")
            return False

    def generate_deployment_report(self, results: Dict[str, Any]) -> str:
        """Generate deployment summary report"""

        report = f"""
LlamaIndex Deployment Report
===========================

Deployment ID: {self.deployment_id}
Started: {self.start_time.isoformat()}
Completed: {datetime.now().isoformat()}
Duration: {(datetime.now() - self.start_time).total_seconds():.1f} seconds

Phase Results:
{json.dumps(results, indent=2, default=str)}

Status: {'SUCCESS' if results.get('deployment_success', False) else 'FAILED'}
        """

        return report


async def main():
    """Main deployment orchestrator"""

    if len(sys.argv) > 1 and sys.argv[1] == '--emergency-rollback':
        deployment = ProductionDeployment()
        success = await deployment.emergency_rollback()
        return 0 if success else 1

    deployment = ProductionDeployment()
    results = {}

    try:
        print("ğŸš€ LlamaIndex Production Deployment")
        print("=" * 50)

        # Validation
        prereq_results = await deployment.validate_prerequisites()
        results['prerequisites'] = prereq_results

        if not all(prereq_results.values()):
            print("\nâŒ Prerequisites not met. Aborting deployment.")
            return 1

        # Phase 1: Testing
        phase1_results = await deployment.phase_1_testing(sample_size=100)
        results['phase1'] = phase1_results

        if not phase1_results.get('passed_quality_gates', False):
            print("\nâŒ Quality gates not passed. Aborting deployment.")
            return 1

        # Phase 2: Selective replacement
        phase2_results = await deployment.phase_2_selective(backfill_days=30)
        results['phase2'] = phase2_results

        if not phase2_results.get('phase_success', False):
            print("\nâŒ Phase 2 failed. Consider rollback.")
            return 1

        # Phase 3: Full replacement
        phase3_results = await deployment.phase_3_full()
        results['phase3'] = phase3_results

        if not phase3_results.get('phase_success', False):
            print("\nâŒ Phase 3 failed. Initiating rollback...")
            await deployment.emergency_rollback()
            return 1

        # Post-deployment validation
        validation_results = await deployment.post_deployment_validation()
        results['validation'] = validation_results

        results['deployment_success'] = True

        # Generate report
        report = deployment.generate_deployment_report(results)

        print("\n" + "=" * 50)
        print("ğŸ‰ DEPLOYMENT SUCCESSFUL!")
        print("\nğŸ“Š Summary:")
        print(f"  Total time: {(datetime.now() - deployment.start_time).total_seconds():.1f}s")
        print(f"  Phase completed: {deployment.phase}")
        print(f"  Articles processed: {results.get('phase1', {}).get('articles_processed', 0)}")

        print("\nğŸ¯ Next steps:")
        print("  1. Monitor system performance for 24 hours")
        print("  2. Review query quality with users")
        print("  3. Adjust cost limits if needed")
        print("  4. Schedule archive migration if not complete")

        # Save report
        with open(f"deployment_report_{deployment.deployment_id}.txt", 'w') as f:
            f.write(report)

        return 0

    except Exception as e:
        print(f"\nğŸ’¥ DEPLOYMENT FAILED: {e}")
        logger.exception("Deployment failed")

        # Attempt rollback
        print("\nğŸš¨ Attempting automatic rollback...")
        rollback_success = await deployment.emergency_rollback()

        if rollback_success:
            print("âœ… Rollback completed. System restored to legacy mode.")
        else:
            print("âŒ Rollback failed. Manual intervention required!")

        return 1


if __name__ == "__main__":
    exit(asyncio.run(main()))