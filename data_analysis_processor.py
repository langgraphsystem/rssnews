#!/usr/bin/env python3
"""
ü§ñ GPT-5 Data Analysis Processor
–†–∞—Å—à–∏—Ä–µ–Ω–Ω–∞—è —Å–∏—Å—Ç–µ–º–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –ø–æ –∫–æ–º–∞–Ω–¥–∞–º GPT-5

–ö–æ–º–∞–Ω–¥—ã:
‚Ä¢ /analyze [query] [timeframe] - Deep data analysis
‚Ä¢ /summarize [topic] [length] - AI-powered summaries
‚Ä¢ /aggregate [metric] [groupby] - Data aggregation
‚Ä¢ /filter [criteria] [value] - Smart filtering
‚Ä¢ /insights [topic] - Business insights generation
‚Ä¢ /sentiment [query] - Sentiment analysis
‚Ä¢ /topics [scope] - Topic modeling & trends
"""

import asyncio
import json
import logging
import os
import sys
from datetime import datetime, timedelta
from typing import Dict, Any, List, Optional, Tuple
from dataclasses import dataclass
from enum import Enum

# Import project modules
try:
    from gpt5_service_new import create_gpt5_service, GPT5Service
    from pg_client_new import PgClient
    from ranking_api import RankingAPI
    from caching_service import CachingService
    from clustering_service import ClusteringService
except ImportError as e:
    logging.error(f"Import error: {e}")
    sys.exit(1)

# Setup logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.StreamHandler(sys.stdout),
        logging.FileHandler('data_analysis.log', encoding='utf-8')
    ]
)
logger = logging.getLogger(__name__)

class AnalysisType(Enum):
    """–¢–∏–ø—ã –∞–Ω–∞–ª–∏–∑–∞ –¥–∞–Ω–Ω—ã—Ö"""
    DEEP_ANALYSIS = "analyze"
    SUMMARY = "summarize"
    AGGREGATION = "aggregate"
    FILTERING = "filter"
    INSIGHTS = "insights"
    SENTIMENT = "sentiment"
    TOPICS = "topics"

@dataclass
class AnalysisRequest:
    """–ó–∞–ø—Ä–æ—Å –Ω–∞ –∞–Ω–∞–ª–∏–∑ –¥–∞–Ω–Ω—ã—Ö"""
    command: AnalysisType
    query: str = ""
    timeframe: str = "7d"
    parameters: Dict[str, Any] = None
    limit: int = 100

    def __post_init__(self):
        if self.parameters is None:
            self.parameters = {}

@dataclass
class AnalysisResult:
    """–†–µ–∑—É–ª—å—Ç–∞—Ç –∞–Ω–∞–ª–∏–∑–∞ –¥–∞–Ω–Ω—ã—Ö"""
    success: bool
    data: Dict[str, Any]
    metadata: Dict[str, Any]
    processing_time: float
    error: Optional[str] = None

class DataAnalysisProcessor:
    """–ì–ª–∞–≤–Ω—ã–π –ø—Ä–æ—Ü–µ—Å—Å–æ—Ä –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ –¥–∞–Ω–Ω—ã—Ö GPT-5"""

    def __init__(self):
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –ø—Ä–æ—Ü–µ—Å—Å–æ—Ä–∞"""
        self.gpt5_service: Optional[GPT5Service] = None
        self.pg_client: Optional[PgClient] = None
        self.ranking_api: Optional[RankingAPI] = None
        self.caching_service: Optional[CachingService] = None
        self.clustering_service: Optional[ClusteringService] = None
        self._initialize_services()

    def _initialize_services(self):
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –≤—Å–µ—Ö —Å–µ—Ä–≤–∏—Å–æ–≤"""
        try:
            # GPT-5 Service
            self.gpt5_service = create_gpt5_service("gpt-5")
            logger.info("‚úÖ GPT-5 service initialized")

            # Database connection
            dsn = os.getenv("PG_DSN")
            if dsn:
                self.pg_client = PgClient(dsn)
                logger.info("‚úÖ PostgreSQL client initialized")

            # Ranking API
            if self.pg_client:
                self.ranking_api = RankingAPI(self.pg_client)
                logger.info("‚úÖ Ranking API initialized")

            # Caching service
            self.caching_service = CachingService()
            logger.info("‚úÖ Caching service initialized")

            # Clustering service
            if self.pg_client:
                self.clustering_service = ClusteringService(self.pg_client)
                logger.info("‚úÖ Clustering service initialized")

        except Exception as e:
            logger.error(f"‚ùå Service initialization error: {e}")

    async def process_analysis_request(self, request: AnalysisRequest) -> AnalysisResult:
        """–û—Å–Ω–æ–≤–Ω–æ–π –º–µ—Ç–æ–¥ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –∑–∞–ø—Ä–æ—Å–∞ –Ω–∞ –∞–Ω–∞–ª–∏–∑"""
        start_time = datetime.now()

        try:
            logger.info(f"üîç Processing {request.command.value} request: {request.query}")

            # –ü–æ–ª—É—á–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö
            articles = await self._get_articles_for_analysis(
                request.query,
                request.timeframe,
                request.limit
            )

            # –í—ã–±–æ—Ä –º–µ—Ç–æ–¥–∞ –∞–Ω–∞–ª–∏–∑–∞
            result_data = await self._route_analysis(request, articles)

            processing_time = (datetime.now() - start_time).total_seconds()

            return AnalysisResult(
                success=True,
                data=result_data,
                metadata={
                    'articles_count': len(articles),
                    'request_type': request.command.value,
                    'query': request.query,
                    'timeframe': request.timeframe,
                    'timestamp': datetime.now().isoformat()
                },
                processing_time=processing_time
            )

        except Exception as e:
            processing_time = (datetime.now() - start_time).total_seconds()
            logger.error(f"‚ùå Analysis processing error: {e}")

            return AnalysisResult(
                success=False,
                data={},
                metadata={
                    'request_type': request.command.value,
                    'query': request.query,
                    'timestamp': datetime.now().isoformat()
                },
                processing_time=processing_time,
                error=str(e)
            )

    async def _get_articles_for_analysis(self, query: str, timeframe: str, limit: int) -> List[Dict[str, Any]]:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ —Å—Ç–∞—Ç–µ–π –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞"""
        try:
            if not self.ranking_api:
                # Fallback mock data for testing
                return self._get_mock_articles(query, limit)

            # –ü—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ timeframe –≤ –¥–Ω–∏
            days = self._parse_timeframe(timeframe)

            # –ü–æ–∏—Å–∫ —á–µ—Ä–µ–∑ Ranking API
            from ranking_api import SearchRequest
            search_request = SearchRequest(
                query=query,
                method="hybrid",
                limit=limit,
                filters={'time_range': f'{days}d'}
            )

            response = await self.ranking_api.search(search_request)

            if response.success and response.results:
                return response.results
            else:
                logger.warning(f"No results from ranking API, using mock data")
                return self._get_mock_articles(query, limit)

        except Exception as e:
            logger.error(f"Error getting articles: {e}")
            return self._get_mock_articles(query, limit)

    def _get_mock_articles(self, query: str, limit: int) -> List[Dict[str, Any]]:
        """Mock –¥–∞–Ω–Ω—ã–µ –¥–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è"""
        mock_articles = [
            {
                'title': f'AI —Ç–µ—Ö–Ω–æ–ª–æ–≥–∏–∏ 2025: –Ω–æ–≤—ã–µ –ø—Ä–æ—Ä—ã–≤—ã –≤ {query}',
                'content': f'–ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω—ã–π –ø—Ä–æ–≥—Ä–µ—Å—Å –≤ –æ–±–ª–∞—Å—Ç–∏ {query}. –ù–æ–≤—ã–µ —Ç–µ—Ö–Ω–æ–ª–æ–≥–∏–∏ –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É—é—Ç —É–ª—É—á—à–µ–Ω–∏–µ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –Ω–∞ 40% –ø–æ —Å—Ä–∞–≤–Ω–µ–Ω–∏—é —Å –ø—Ä–µ–¥—ã–¥—É—â–∏–º–∏ —Ä–µ—à–µ–Ω–∏—è–º–∏...',
                'source': 'tech.com',
                'source_domain': 'tech.com',
                'published_at': '2025-09-25T10:00:00',
                'score': 0.95,
                'tags': ['AI', 'technology', 'innovation']
            },
            {
                'title': f'–†—ã–Ω–æ—á–Ω—ã–µ —Ç–µ–Ω–¥–µ–Ω—Ü–∏–∏ {query}: –∞–Ω–∞–ª–∏–∑ —ç–∫—Å–ø–µ—Ä—Ç–æ–≤',
                'content': f'–ê–Ω–∞–ª–∏—Ç–∏–∫–∏ –ø—Ä–æ–≥–Ω–æ–∑–∏—Ä—É—é—Ç —Ä–æ—Å—Ç —Ä—ã–Ω–∫–∞ {query} –Ω–∞ 25% –≤ —Å–ª–µ–¥—É—é—â–µ–º –≥–æ–¥—É. –û—Å–Ω–æ–≤–Ω—ã–º–∏ –¥—Ä–∞–π–≤–µ—Ä–∞–º–∏ —Ä–æ—Å—Ç–∞ —è–≤–ª—è—é—Ç—Å—è –∏–Ω–Ω–æ–≤–∞—Ü–∏–∏ –∏ —É–≤–µ–ª–∏—á–µ–Ω–∏–µ —Å–ø—Ä–æ—Å–∞...',
                'source': 'market.news',
                'source_domain': 'market.news',
                'published_at': '2025-09-25T09:30:00',
                'score': 0.88,
                'tags': ['market', 'trends', 'growth']
            },
            {
                'title': f'–ë—É–¥—É—â–µ–µ {query}: –ø—Ä–æ–≥–Ω–æ–∑—ã –∏ —Ä–µ–∞–ª—å–Ω–æ—Å—Ç—å',
                'content': f'–≠–∫—Å–ø–µ—Ä—Ç–Ω–æ–µ –º–Ω–µ–Ω–∏–µ –æ —Ä–∞–∑–≤–∏—Ç–∏–∏ {query} –≤–∫–ª—é—á–∞–µ—Ç –∞–Ω–∞–ª–∏–∑ —Ç–µ–∫—É—â–∏—Ö —Ç—Ä–µ–Ω–¥–æ–≤ –∏ –±—É–¥—É—â–∏—Ö –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–µ–π. –ö–ª—é—á–µ–≤—ã–µ —Ñ–∞–∫—Ç–æ—Ä—ã –≤–ª–∏—è–Ω–∏—è...',
                'source': 'future.tech',
                'source_domain': 'future.tech',
                'published_at': '2025-09-25T08:45:00',
                'score': 0.82,
                'tags': ['future', 'predictions', 'analysis']
            }
        ]

        return mock_articles[:min(limit, len(mock_articles))]

    def _parse_timeframe(self, timeframe: str) -> int:
        """–ü–∞—Ä—Å–∏–Ω–≥ –≤—Ä–µ–º–µ–Ω–Ω–æ–≥–æ –¥–∏–∞–ø–∞–∑–æ–Ω–∞ –≤ –¥–Ω–∏"""
        timeframe = timeframe.lower().strip()

        if timeframe.endswith('d'):
            return int(timeframe[:-1])
        elif timeframe.endswith('w'):
            return int(timeframe[:-1]) * 7
        elif timeframe.endswith('m'):
            return int(timeframe[:-1]) * 30
        else:
            try:
                return int(timeframe)
            except ValueError:
                return 7  # default 1 week

    async def _route_analysis(self, request: AnalysisRequest, articles: List[Dict[str, Any]]) -> Dict[str, Any]:
        """–ú–∞—Ä—à—Ä—É—Ç–∏–∑–∞—Ü–∏—è –∫ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—â–µ–º—É –º–µ—Ç–æ–¥—É –∞–Ω–∞–ª–∏–∑–∞"""

        if request.command == AnalysisType.DEEP_ANALYSIS:
            return await self._process_deep_analysis(request, articles)
        elif request.command == AnalysisType.SUMMARY:
            return await self._process_summary(request, articles)
        elif request.command == AnalysisType.AGGREGATION:
            return await self._process_aggregation(request, articles)
        elif request.command == AnalysisType.FILTERING:
            return await self._process_filtering(request, articles)
        elif request.command == AnalysisType.INSIGHTS:
            return await self._process_insights(request, articles)
        elif request.command == AnalysisType.SENTIMENT:
            return await self._process_sentiment(request, articles)
        elif request.command == AnalysisType.TOPICS:
            return await self._process_topics(request, articles)
        else:
            raise ValueError(f"Unknown analysis type: {request.command}")

    async def _process_deep_analysis(self, request: AnalysisRequest, articles: List[Dict[str, Any]]) -> Dict[str, Any]:
        """–ì–ª—É–±–æ–∫–∏–π –∞–Ω–∞–ª–∏–∑ –¥–∞–Ω–Ω—ã—Ö"""
        logger.info(f"üß† Processing deep analysis for: {request.query}")

        # –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è GPT-5
        articles_text = self._format_articles_for_gpt(articles, include_metadata=True)

        prompt = f"""–ü—Ä–æ–≤–µ–¥–∏ –≥–ª—É–±–æ–∫–∏–π –∫–æ–º–ø–ª–µ–∫—Å–Ω—ã–π –∞–Ω–∞–ª–∏–∑ {len(articles)} —Å—Ç–∞—Ç–µ–π –ø–æ —Ç–µ–º–µ '{request.query}':

–î–ê–ù–ù–´–ï –î–õ–Ø –ê–ù–ê–õ–ò–ó–ê:
{articles_text}

–¢–†–ï–ë–£–ï–ú–´–ô –ê–ù–ê–õ–ò–ó:
1. **–ö–õ–Æ–ß–ï–í–´–ï –¢–†–ï–ù–î–´ –ò –ü–ê–¢–¢–ï–†–ù–´**
   - –í—ã—è–≤–ª–µ–Ω–∏–µ –æ—Å–Ω–æ–≤–Ω—ã—Ö —Ç–µ–Ω–¥–µ–Ω—Ü–∏–π
   - –ê–Ω–∞–ª–∏–∑ –∑–∞–∫–æ–Ω–æ–º–µ—Ä–Ω–æ—Å—Ç–µ–π –≤ –¥–∞–Ω–Ω—ã—Ö
   - –í—Ä–µ–º–µ–Ω–Ω–∞—è –¥–∏–Ω–∞–º–∏–∫–∞ –∏–∑–º–µ–Ω–µ–Ω–∏–π

2. **–°–¢–ê–¢–ò–°–¢–ò–ß–ï–°–ö–ò–ï –ò–ù–°–ê–ô–¢–´**
   - –ö–æ–ª–∏—á–µ—Å—Ç–≤–µ–Ω–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏
   - –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –ø–æ –∏—Å—Ç–æ—á–Ω–∏–∫–∞–º
   - –ß–∞—Å—Ç–æ—Ç–∞ —É–ø–æ–º–∏–Ω–∞–Ω–∏–π –∫–ª—é—á–µ–≤—ã—Ö —Ç–µ—Ä–º–∏–Ω–æ–≤

3. **–ö–ê–ß–ï–°–¢–í–ï–ù–ù–´–ô –ê–ù–ê–õ–ò–ó**
   - –ì–ª—É–±–∏–Ω–∞ –æ—Å–≤–µ—â–µ–Ω–∏—è —Ç–µ–º—ã
   - –≠–∫—Å–ø–µ—Ä—Ç–Ω—ã–µ –º–Ω–µ–Ω–∏—è –∏ –ø—Ä–æ–≥–Ω–æ–∑—ã
   - –ü—Ä–æ—Ç–∏–≤–æ—Ä–µ—á–∏—è –∏ —Å–ø–æ—Ä–Ω—ã–µ –º–æ–º–µ–Ω—Ç—ã

4. **–ü–†–û–ì–ù–û–°–¢–ò–ß–ï–°–ö–ê–Ø –ê–ù–ê–õ–ò–¢–ò–ö–ê**
   - –ö—Ä–∞—Ç–∫–æ—Å—Ä–æ—á–Ω—ã–µ –ø—Ä–æ–≥–Ω–æ–∑—ã (1-3 –º–µ—Å—è—Ü–∞)
   - –î–æ–ª–≥–æ—Å—Ä–æ—á–Ω—ã–µ —Ç—Ä–µ–Ω–¥—ã (6-12 –º–µ—Å—è—Ü–µ–≤)
   - –§–∞–∫—Ç–æ—Ä—ã —Ä–∏—Å–∫–∞ –∏ –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏

5. **–†–ï–ö–û–ú–ï–ù–î–ê–¶–ò–ò**
   - –°—Ç—Ä–∞—Ç–µ–≥–∏—á–µ—Å–∫–∏–µ –≤—ã–≤–æ–¥—ã
   - –ü—Ä–∞–∫—Ç–∏—á–µ—Å–∫–∏–µ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏
   - –û–±–ª–∞—Å—Ç–∏ –¥–ª—è –¥–∞–ª—å–Ω–µ–π—à–µ–≥–æ –∏–∑—É—á–µ–Ω–∏—è

–ò—Å–ø–æ–ª—å–∑—É–π —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–π —Ñ–æ—Ä–º–∞—Ç —Å —Ç–∞–±–ª–∏—Ü–∞–º–∏, –≥—Ä–∞—Ñ–∏–∫–∞–º–∏ (—á–µ—Ä–µ–∑ —ç–º–æ–¥–∑–∏) –∏ –≤–∏–∑—É–∞–ª—å–Ω—ã–º–∏ —ç–ª–µ–º–µ–Ω—Ç–∞–º–∏."""

        try:
            analysis_result = self.gpt5_service.send_analysis(
                prompt,
                max_output_tokens=2000,
                verbosity="high",
                reasoning_effort="high"
            )

            # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏
            metrics = self._calculate_analysis_metrics(articles)

            return {
                'analysis': analysis_result,
                'metrics': metrics,
                'summary_stats': {
                    'total_articles': len(articles),
                    'date_range': self._get_date_range(articles),
                    'top_sources': self._get_top_sources(articles),
                    'avg_score': self._get_average_score(articles)
                }
            }

        except Exception as e:
            logger.error(f"‚ùå Deep analysis error: {e}")
            return {'error': str(e), 'analysis': '–ê–Ω–∞–ª–∏–∑ –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω'}

    async def _process_summary(self, request: AnalysisRequest, articles: List[Dict[str, Any]]) -> Dict[str, Any]:
        """AI-powered —Å—É–º–º–∞—Ä–∏–∑–∞—Ü–∏—è"""
        logger.info(f"üìã Processing summary for: {request.query}")

        length = request.parameters.get('length', 'medium')
        articles_text = self._format_articles_for_gpt(articles, include_metadata=True)

        # –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Ä–∞–∑–º–µ—Ä–∞ summary –ø–æ –ø–∞—Ä–∞–º–µ—Ç—Ä—É length
        token_limits = {
            'short': 500,
            'medium': 1000,
            'long': 1500,
            'detailed': 2000
        }
        max_tokens = token_limits.get(length, 1000)

        prompt = f"""–°–æ–∑–¥–∞–π –ø—Ä–æ—Ñ–µ—Å—Å–∏–æ–Ω–∞–ª—å–Ω—É—é —Å–≤–æ–¥–∫—É {length} –¥–ª–∏–Ω—ã –ø–æ {len(articles)} —Å—Ç–∞—Ç—å—è–º –Ω–∞ —Ç–µ–º—É '{request.query}':

–ò–°–•–û–î–ù–´–ï –î–ê–ù–ù–´–ï:
{articles_text}

–¢–†–ï–ë–û–í–ê–ù–ò–Ø –ö –°–í–û–î–ö–ï:
- –†–∞–∑–º–µ—Ä: {length}
- –°—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–π —Ñ–æ—Ä–º–∞—Ç
- –í—ã–¥–µ–ª–µ–Ω–∏–µ –∫–ª—é—á–µ–≤—ã—Ö —Ñ–∞–∫—Ç–æ–≤
- –•—Ä–æ–Ω–æ–ª–æ–≥–∏—á–µ—Å–∫–∏–π –ø–æ—Ä—è–¥–æ–∫ —Å–æ–±—ã—Ç–∏–π
- –í–∞–∂–Ω—ã–µ —Ü–∏—Ñ—Ä—ã –∏ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞

–°–¢–†–£–ö–¢–£–†–ê –°–í–û–î–ö–ò:
1. **EXECUTIVE SUMMARY**
   - –û—Å–Ω–æ–≤–Ω—ã–µ –≤—ã–≤–æ–¥—ã (2-3 –ø—É–Ω–∫—Ç–∞)
   - –ö–ª—é—á–µ–≤—ã–µ —Ü–∏—Ñ—Ä—ã

2. **–û–°–ù–û–í–ù–´–ï –°–û–ë–´–¢–ò–Ø**
   - –í–∞–∂–Ω–µ–π—à–∏–µ –Ω–æ–≤–æ—Å—Ç–∏
   - –í—Ä–µ–º–µ–Ω–Ω–∞—è –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç—å

3. **–ê–ù–ê–õ–ò–¢–ò–ö–ê**
   - –¢—Ä–µ–Ω–¥—ã –∏ –∑–∞–∫–æ–Ω–æ–º–µ—Ä–Ω–æ—Å—Ç–∏
   - –ú–Ω–µ–Ω–∏—è —ç–∫—Å–ø–µ—Ä—Ç–æ–≤

4. **–ó–ê–ö–õ–Æ–ß–ï–ù–ò–ï**
   - –û–±—â–∏–µ –≤—ã–≤–æ–¥—ã
   - –ó–Ω–∞—á–∏–º–æ—Å—Ç—å –¥–ª—è –æ—Ç—Ä–∞—Å–ª–∏

–ò—Å–ø–æ–ª—å–∑—É–π —è—Å–Ω—ã–π –¥–µ–ª–æ–≤–æ–π —Å—Ç–∏–ª—å —Å –º–∞—Ä–∫–∏—Ä–æ–≤–∞–Ω–Ω—ã–º–∏ —Å–ø–∏—Å–∫–∞–º–∏."""

        try:
            summary_result = self.gpt5_service.send_chat(
                prompt,
                max_output_tokens=max_tokens,
                verbosity="medium"
            )

            return {
                'summary': summary_result,
                'length': length,
                'word_count': len(summary_result.split()) if summary_result else 0,
                'articles_processed': len(articles),
                'coverage_stats': self._calculate_coverage_stats(articles)
            }

        except Exception as e:
            logger.error(f"‚ùå Summary error: {e}")
            return {'error': str(e), 'summary': '–°–≤–æ–¥–∫–∞ –Ω–µ–¥–æ—Å—Ç—É–ø–Ω–∞'}

    async def _process_aggregation(self, request: AnalysisRequest, articles: List[Dict[str, Any]]) -> Dict[str, Any]:
        """–ê–≥—Ä–µ–≥–∞—Ü–∏—è –¥–∞–Ω–Ω—ã—Ö"""
        logger.info(f"üìä Processing aggregation for: {request.query}")

        metric = request.parameters.get('metric', '–∏—Å—Ç–æ—á–Ω–∏–∫–∏')
        groupby = request.parameters.get('groupby', '–¥–∞—Ç–∞')

        # –ë–∞–∑–æ–≤–∞—è –∞–≥—Ä–µ–≥–∞—Ü–∏—è –¥–∞–Ω–Ω—ã—Ö
        aggregated_data = self._perform_data_aggregation(articles, metric, groupby)

        # GPT-5 –∞–Ω–∞–ª–∏–∑ –∞–≥—Ä–µ–≥–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö
        articles_text = self._format_articles_for_gpt(articles, include_metadata=True)

        prompt = f"""–ü—Ä–æ–≤–µ–¥–∏ –∞–≥—Ä–µ–≥–∞—Ü–∏—é –∏ –∞–Ω–∞–ª–∏–∑ –¥–∞–Ω–Ω—ã—Ö –ø–æ {len(articles)} —Å—Ç–∞—Ç—å—è–º:

–î–ê–ù–ù–´–ï:
{articles_text}

–ó–ê–î–ê–ß–ê –ê–ì–†–ï–ì–ê–¶–ò–ò:
- –ú–µ—Ç—Ä–∏–∫–∞: {metric}
- –ì—Ä—É–ø–ø–∏—Ä–æ–≤–∫–∞: {groupby}

–¢–†–ï–ë–£–ï–ú–´–ô –ê–ù–ê–õ–ò–ó:
1. **–ö–û–õ–ò–ß–ï–°–¢–í–ï–ù–ù–ê–Ø –ê–ì–†–ï–ì–ê–¶–ò–Ø**
   - –ü–æ–¥—Å—á–µ—Ç –ø–æ –∫–∞—Ç–µ–≥–æ—Ä–∏—è–º
   - –ü—Ä–æ—Ü–µ–Ω—Ç–Ω–æ–µ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ
   - –°—Ç–∞—Ç–∏—Å—Ç–∏—á–µ—Å–∫–∏–µ –ø–æ–∫–∞–∑–∞—Ç–µ–ª–∏

2. **–í–†–ï–ú–ï–ù–ù–û–ô –ê–ù–ê–õ–ò–ó**
   - –î–∏–Ω–∞–º–∏–∫–∞ –ø–æ –≤—Ä–µ–º–µ–Ω–∏
   - –ü–∏–∫–æ–≤—ã–µ –ø–µ—Ä–∏–æ–¥—ã
   - –¢—Ä–µ–Ω–¥—ã —Ä–æ—Å—Ç–∞/—Å–ø–∞–¥–∞

3. **–°–†–ê–í–ù–ò–¢–ï–õ–¨–ù–´–ô –ê–ù–ê–õ–ò–ó**
   - –¢–æ–ø –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤/–∫–∞—Ç–µ–≥–æ—Ä–∏–π
   - –°—Ä–∞–≤–Ω–µ–Ω–∏–µ –ø–æ–∫–∞–∑–∞—Ç–µ–ª–µ–π
   - –í—ã—è–≤–ª–µ–Ω–∏–µ –∞–Ω–æ–º–∞–ª–∏–π

4. **–í–ò–ó–£–ê–õ–ò–ó–ê–¶–ò–Ø –î–ê–ù–ù–´–•**
   - –¢–∞–±–ª–∏—Ü—ã —Å —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º–∏
   - –ì—Ä–∞—Ñ–∏–∫–∏ (—Ç–µ–∫—Å—Ç–æ–≤—ã–µ)
   - –î–∏–∞–≥—Ä–∞–º–º—ã —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è

–ü—Ä–µ–¥—Å—Ç–∞–≤—å —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –≤ —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω–æ–º –≤–∏–¥–µ —Å —á–µ—Ç–∫–∏–º–∏ —Ç–∞–±–ª–∏—Ü–∞–º–∏ –∏ –≥—Ä–∞—Ñ–∏–∫–∞–º–∏."""

        try:
            analysis_result = self.gpt5_service.send_bulk(
                prompt,
                max_output_tokens=1200,
                verbosity="high"
            )

            return {
                'aggregation_analysis': analysis_result,
                'raw_aggregation': aggregated_data,
                'metric': metric,
                'groupby': groupby,
                'total_items': len(articles)
            }

        except Exception as e:
            logger.error(f"‚ùå Aggregation error: {e}")
            return {'error': str(e), 'aggregation': '–ê–≥—Ä–µ–≥–∞—Ü–∏—è –Ω–µ–¥–æ—Å—Ç—É–ø–Ω–∞'}

    async def _process_filtering(self, request: AnalysisRequest, articles: List[Dict[str, Any]]) -> Dict[str, Any]:
        """–£–º–Ω–∞—è —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏—è –¥–∞–Ω–Ω—ã—Ö"""
        logger.info(f"üîç Processing filtering for: {request.query}")

        criteria = request.parameters.get('criteria', '–∏—Å—Ç–æ—á–Ω–∏–∫')
        value = request.parameters.get('value', 'tech.com')

        # –ü—Ä–∏–º–µ–Ω–µ–Ω–∏–µ —Ñ–∏–ª—å—Ç—Ä–æ–≤
        filtered_articles = self._apply_smart_filters(articles, criteria, value)

        articles_text = self._format_articles_for_gpt(articles, include_metadata=True)
        filtered_text = self._format_articles_for_gpt(filtered_articles, include_metadata=True)

        prompt = f"""–í—ã–ø–æ–ª–Ω–∏ —É–º–Ω—É—é —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏—é –∏ –∞–Ω–∞–ª–∏–∑ –¥–∞–Ω–Ω—ã—Ö:

–ò–°–•–û–î–ù–´–ï –î–ê–ù–ù–´–ï ({len(articles)} —Å—Ç–∞—Ç–µ–π):
{articles_text}

–û–¢–§–ò–õ–¨–¢–†–û–í–ê–ù–ù–´–ï –î–ê–ù–ù–´–ï ({len(filtered_articles)} —Å—Ç–∞—Ç–µ–π):
{filtered_text}

–ö–†–ò–¢–ï–†–ò–ò –§–ò–õ–¨–¢–†–ê–¶–ò–ò:
- –ü–∞—Ä–∞–º–µ—Ç—Ä: {criteria}
- –ó–Ω–∞—á–µ–Ω–∏–µ: {value}

–ê–ù–ê–õ–ò–ó –§–ò–õ–¨–¢–†–ê–¶–ò–ò:
1. **–†–ï–ó–£–õ–¨–¢–ê–¢–´ –§–ò–õ–¨–¢–†–ê–¶–ò–ò**
   - –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –Ω–∞–π–¥–µ–Ω–Ω—ã—Ö —Å—Ç–∞—Ç–µ–π
   - –ü—Ä–æ—Ü–µ–Ω—Ç –æ—Ç –æ–±—â–µ–≥–æ –æ–±—ä–µ–º–∞
   - –†–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç—å —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤

2. **–ö–ê–ß–ï–°–¢–í–ï–ù–ù–´–ô –ê–ù–ê–õ–ò–ó**
   - –°–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–µ –∫—Ä–∏—Ç–µ—Ä–∏—è–º
   - –¢–æ—á–Ω–æ—Å—Ç—å —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏
   - –ü—Ä–æ–ø—É—â–µ–Ω–Ω—ã–µ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–µ –º–∞—Ç–µ—Ä–∏–∞–ª—ã

3. **–°–û–î–ï–†–ñ–ê–¢–ï–õ–¨–ù–´–ô –ê–ù–ê–õ–ò–ó**
   - –û—Å–Ω–æ–≤–Ω—ã–µ —Ç–µ–º—ã –≤ –æ—Ç—Ñ–∏–ª—å—Ç—Ä–æ–≤–∞–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö
   - –£–Ω–∏–∫–∞–ª—å–Ω—ã–µ –∏–Ω—Å–∞–π—Ç—ã
   - –ö–ª—é—á–µ–≤—ã–µ –≤—ã–≤–æ–¥—ã

4. **–†–ï–ö–û–ú–ï–ù–î–ê–¶–ò–ò –ü–û –§–ò–õ–¨–¢–†–ê–¶–ò–ò**
   - –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –ø–æ–ª–µ–∑–Ω—ã–µ —Ñ–∏–ª—å—Ç—Ä—ã
   - –£–ª—É—á—à–µ–Ω–∏—è –∫—Ä–∏—Ç–µ—Ä–∏–µ–≤ –ø–æ–∏—Å–∫–∞
   - –ê–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–Ω—ã–µ –ø–æ–¥—Ö–æ–¥—ã

–ü—Ä–µ–¥–æ—Å—Ç–∞–≤—å –¥–µ—Ç–∞–ª—å–Ω—ã–π –∞–Ω–∞–ª–∏–∑ —Å –ø—Ä–∏–º–µ—Ä–∞–º–∏ –∏ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏—è–º–∏."""

        try:
            analysis_result = self.gpt5_service.send_chat(
                prompt,
                max_output_tokens=1200,
                verbosity="medium"
            )

            return {
                'filtering_analysis': analysis_result,
                'filtered_articles': filtered_articles,
                'filter_stats': {
                    'criteria': criteria,
                    'value': value,
                    'original_count': len(articles),
                    'filtered_count': len(filtered_articles),
                    'filter_ratio': len(filtered_articles) / len(articles) if articles else 0
                }
            }

        except Exception as e:
            logger.error(f"‚ùå Filtering error: {e}")
            return {'error': str(e), 'filtering': '–§–∏–ª—å—Ç—Ä–∞—Ü–∏—è –Ω–µ–¥–æ—Å—Ç—É–ø–Ω–∞'}

    async def _process_insights(self, request: AnalysisRequest, articles: List[Dict[str, Any]]) -> Dict[str, Any]:
        """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –±–∏–∑–Ω–µ—Å-–∏–Ω—Å–∞–π—Ç–æ–≤"""
        logger.info(f"üí° Processing insights for: {request.query}")

        articles_text = self._format_articles_for_gpt(articles, include_metadata=True)

        prompt = f"""–°–≥–µ–Ω–µ—Ä–∏—Ä—É–π –≥–ª—É–±–æ–∫–∏–µ –±–∏–∑–Ω–µ—Å-–∏–Ω—Å–∞–π—Ç—ã –Ω–∞ –æ—Å–Ω–æ–≤–µ {len(articles)} —Å—Ç–∞—Ç–µ–π –ø–æ —Ç–µ–º–µ '{request.query}':

–ò–°–¢–û–ß–ù–ò–ö–ò –î–ê–ù–ù–´–•:
{articles_text}

–¢–†–ï–ë–£–ï–ú–´–ï –ò–ù–°–ê–ô–¢–´:
1. **–†–´–ù–û–ß–ù–ê–Ø –ê–ù–ê–õ–ò–¢–ò–ö–ê**
   - –°–∫—Ä—ã—Ç—ã–µ —Ç—Ä–µ–Ω–¥—ã –∏ –ø–∞—Ç—Ç–µ—Ä–Ω—ã
   - –ò–∑–º–µ–Ω–µ–Ω–∏—è –≤ –∫–æ–Ω–∫—É—Ä–µ–Ω—Ç–Ω–æ–π —Å—Ä–µ–¥–µ
   - –ù–æ–≤—ã–µ –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏ –∏ —É–≥—Ä–æ–∑—ã
   - –î–≤–∏–∂—É—â–∏–µ —Å–∏–ª—ã —Ä—ã–Ω–∫–∞

2. **–ü–†–û–ì–ù–û–°–¢–ò–ß–ï–°–ö–ê–Ø –ê–ù–ê–õ–ò–¢–ò–ö–ê**
   - –ö—Ä–∞—Ç–∫–æ—Å—Ä–æ—á–Ω—ã–µ –ø—Ä–æ–≥–Ω–æ–∑—ã (1-3 –º–µ—Å—è—Ü–∞)
   - –î–æ–ª–≥–æ—Å—Ä–æ—á–Ω—ã–µ –ø–µ—Ä—Å–ø–µ–∫—Ç–∏–≤—ã (6-12 –º–µ—Å—è—Ü–µ–≤)
   - –§–∞–∫—Ç–æ—Ä—ã —Ä–∏—Å–∫–∞ –∏ –∫–∞—Ç–∞–ª–∏–∑–∞—Ç–æ—Ä—ã
   - –°—Ü–µ–Ω–∞—Ä–∏–∏ —Ä–∞–∑–≤–∏—Ç–∏—è —Å–æ–±—ã—Ç–∏–π

3. **–°–¢–†–ê–¢–ï–ì–ò–ß–ï–°–ö–ò–ï –†–ï–ö–û–ú–ï–ù–î–ê–¶–ò–ò**
   - –ü—Ä–∞–∫—Ç–∏—á–µ—Å–∫–∏–µ –±–∏–∑–Ω–µ—Å-–∏–Ω—Å–∞–π—Ç—ã
   - –ò–Ω–≤–µ—Å—Ç–∏—Ü–∏–æ–Ω–Ω—ã–µ –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏
   - –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –ø–æ –ø–æ–∑–∏—Ü–∏–æ–Ω–∏—Ä–æ–≤–∞–Ω–∏—é
   - –û–ø–µ—Ä–∞—Ü–∏–æ–Ω–Ω—ã–µ –≤—ã–≤–æ–¥—ã

4. **–ò–ù–î–ò–ö–ê–¢–û–†–´ –≠–§–§–ï–ö–¢–ò–í–ù–û–°–¢–ò**
   - –ö–ª—é—á–µ–≤—ã–µ –º–µ—Ç—Ä–∏–∫–∏ –¥–ª—è –æ—Ç—Å–ª–µ–∂–∏–≤–∞–Ω–∏—è
   - Benchmarks –∏ —Å—Ä–∞–≤–Ω–µ–Ω–∏—è
   - –°–∏–≥–Ω–∞–ª—ã —Ä–∞–Ω–Ω–µ–≥–æ –ø—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏—è

–ü—Ä–µ–¥—Å—Ç–∞–≤—å –∫–∞–∫ executive briefing —Å —á–µ—Ç–∫–∏–º–∏ —Ä–∞–∑–¥–µ–ª–∞–º–∏ –∏ –ø—Ä–∞–∫—Ç–∏—á–µ—Å–∫–∏–º–∏ –≤—ã–≤–æ–¥–∞–º–∏."""

        try:
            insights_result = self.gpt5_service.send_insights(
                prompt,
                max_output_tokens=1500,
                verbosity="high",
                reasoning_effort="high"
            )

            # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏ –¥–ª—è –∏–Ω—Å–∞–π—Ç–æ–≤
            business_metrics = self._calculate_business_metrics(articles)

            return {
                'insights': insights_result,
                'business_metrics': business_metrics,
                'confidence_score': self._calculate_confidence_score(articles),
                'market_indicators': self._extract_market_indicators(articles)
            }

        except Exception as e:
            logger.error(f"‚ùå Insights error: {e}")
            return {'error': str(e), 'insights': '–ò–Ω—Å–∞–π—Ç—ã –Ω–µ–¥–æ—Å—Ç—É–ø–Ω—ã'}

    async def _process_sentiment(self, request: AnalysisRequest, articles: List[Dict[str, Any]]) -> Dict[str, Any]:
        """–ê–Ω–∞–ª–∏–∑ —Ç–æ–Ω–∞–ª—å–Ω–æ—Å—Ç–∏"""
        logger.info(f"üòä Processing sentiment for: {request.query}")

        articles_text = self._format_articles_for_gpt(articles, include_metadata=True)

        prompt = f"""–ü—Ä–æ–≤–µ–¥–∏ –∫–æ–º–ø–ª–µ–∫—Å–Ω—ã–π –∞–Ω–∞–ª–∏–∑ —Ç–æ–Ω–∞–ª—å–Ω–æ—Å—Ç–∏ {len(articles)} —Å—Ç–∞—Ç–µ–π –ø–æ —Ç–µ–º–µ '{request.query}':

–î–ê–ù–ù–´–ï –î–õ–Ø –ê–ù–ê–õ–ò–ó–ê:
{articles_text}

–¢–†–ï–ë–£–ï–ú–´–ô –ê–ù–ê–õ–ò–ó –¢–û–ù–ê–õ–¨–ù–û–°–¢–ò:
1. **–û–ë–©–ê–Ø –û–¶–ï–ù–ö–ê –¢–û–ù–ê–õ–¨–ù–û–°–¢–ò**
   - –ò—Ç–æ–≥–æ–≤—ã–π sentiment score (-100 –¥–æ +100)
   - –î–æ–≤–µ—Ä–∏—Ç–µ–ª—å–Ω—ã–π –∏–Ω—Ç–µ—Ä–≤–∞–ª –æ—Ü–µ–Ω–∫–∏
   - –°—Ç–µ–ø–µ–Ω—å —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç–∏ –≤ –∞–Ω–∞–ª–∏–∑–µ

2. **–†–ê–°–ü–†–ï–î–ï–õ–ï–ù–ò–ï –¢–û–ù–ê–õ–¨–ù–û–°–¢–ò**
   - –ü–æ–∑–∏—Ç–∏–≤–Ω—ã–µ: X% (—Å –æ–±–æ—Å–Ω–æ–≤–∞–Ω–∏–µ–º)
   - –ù–µ–π—Ç—Ä–∞–ª—å–Ω—ã–µ: X% (—Å –æ–±–æ—Å–Ω–æ–≤–∞–Ω–∏–µ–º)
   - –ù–µ–≥–∞—Ç–∏–≤–Ω—ã–µ: X% (—Å –æ–±–æ—Å–Ω–æ–≤–∞–Ω–∏–µ–º)
   - –°–º–µ—à–∞–Ω–Ω—ã–µ: X% (—Å –æ–±–æ—Å–Ω–æ–≤–∞–Ω–∏–µ–º)

3. **–§–ê–ö–¢–û–†–´ –¢–û–ù–ê–õ–¨–ù–û–°–¢–ò**
   - –ö–ª—é—á–µ–≤—ã–µ –ø–æ–∑–∏—Ç–∏–≤–Ω—ã–µ —Ñ–∞–∫—Ç–æ—Ä—ã
   - –û—Å–Ω–æ–≤–Ω—ã–µ –Ω–µ–≥–∞—Ç–∏–≤–Ω—ã–µ –∞—Å–ø–µ–∫—Ç—ã
   - –ù–µ–π—Ç—Ä–∞–ª—å–Ω—ã–µ/—Å–º–µ—à–∞–Ω–Ω—ã–µ —Å–∏–≥–Ω–∞–ª—ã
   - –≠–º–æ—Ü–∏–æ–Ω–∞–ª—å–Ω—ã–µ –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä—ã

4. **–î–ò–ù–ê–ú–ò–ö–ê –ù–ê–°–¢–†–û–ï–ù–ò–ô**
   - –ò–∑–º–µ–Ω–µ–Ω–∏—è –≤–æ –≤—Ä–µ–º–µ–Ω–∏
   - –¢—Ä–∏–≥–≥–µ—Ä—ã –∏–∑–º–µ–Ω–µ–Ω–∏–π
   - –¢—Ä–µ–Ω–¥—ã —Ä–∞–∑–≤–∏—Ç–∏—è –Ω–∞—Å—Ç—Ä–æ–µ–Ω–∏–π

5. **–í–õ–ò–Ø–ù–ò–ï –ù–ê –†–´–ù–û–ö**
   - –í–æ–∑–¥–µ–π—Å—Ç–≤–∏–µ –Ω–∞ –≤–æ—Å–ø—Ä–∏—è—Ç–∏–µ
   - –í–ª–∏—è–Ω–∏–µ –Ω–∞ —Ä–µ—à–µ–Ω–∏—è –∏–Ω–≤–µ—Å—Ç–æ—Ä–æ–≤
   - –ü—Ä–æ–≥–Ω–æ–∑ –∏–∑–º–µ–Ω–µ–Ω–∏–π —Ç–æ–Ω–∞–ª—å–Ω–æ—Å—Ç–∏

–ò—Å–ø–æ–ª—å–∑—É–π —ç–º–æ–¥–∑–∏ –∏ —á–µ—Ç–∫–æ–µ —Ñ–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –¥–ª—è –Ω–∞–≥–ª—è–¥–Ω–æ—Å—Ç–∏."""

        try:
            sentiment_result = self.gpt5_service.send_sentiment(
                prompt,
                max_output_tokens=1200,
                verbosity="high"
            )

            # –ë–∞–∑–æ–≤—ã–π –∞–Ω–∞–ª–∏–∑ —Ç–æ–Ω–∞–ª—å–Ω–æ—Å—Ç–∏
            sentiment_metrics = self._calculate_sentiment_metrics(articles)

            return {
                'sentiment_analysis': sentiment_result,
                'sentiment_metrics': sentiment_metrics,
                'emotional_indicators': self._extract_emotional_indicators(articles),
                'confidence_level': self._calculate_sentiment_confidence(articles)
            }

        except Exception as e:
            logger.error(f"‚ùå Sentiment error: {e}")
            return {'error': str(e), 'sentiment': '–ê–Ω–∞–ª–∏–∑ —Ç–æ–Ω–∞–ª—å–Ω–æ—Å—Ç–∏ –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω'}

    async def _process_topics(self, request: AnalysisRequest, articles: List[Dict[str, Any]]) -> Dict[str, Any]:
        """–ú–æ–¥–µ–ª–∏—Ä–æ–≤–∞–Ω–∏–µ —Ç–µ–º –∏ –∞–Ω–∞–ª–∏–∑ —Ç—Ä–µ–Ω–¥–æ–≤"""
        logger.info(f"üè∑Ô∏è Processing topics for: {request.query}")

        scope = request.parameters.get('scope', request.query)
        articles_text = self._format_articles_for_gpt(articles, include_metadata=True)

        prompt = f"""–ü—Ä–æ–≤–µ–¥–∏ –º–æ–¥–µ–ª–∏—Ä–æ–≤–∞–Ω–∏–µ —Ç–µ–º –∏ –∞–Ω–∞–ª–∏–∑ —Ç—Ä–µ–Ω–¥–æ–≤ –¥–ª—è {len(articles)} —Å—Ç–∞—Ç–µ–π –≤ –æ–±–ª–∞—Å—Ç–∏ '{scope}':

–î–ê–ù–ù–´–ï –î–õ–Ø –ê–ù–ê–õ–ò–ó–ê:
{articles_text}

–ó–ê–î–ê–ß–ò –ê–ù–ê–õ–ò–ó–ê:
1. **–í–´–Ø–í–õ–ï–ù–ò–ï –û–°–ù–û–í–ù–´–• –¢–ï–ú**
   - –¢–æ–ø-7 –∫–ª—é—á–µ–≤—ã—Ö —Ç–µ–º —Å –æ–ø–∏—Å–∞–Ω–∏—è–º–∏
   - –í–µ—Å–æ–≤—ã–µ –∫–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç—ã —Ç–µ–º (—á–∞—Å—Ç–æ—Ç–∞/–≤–∞–∂–Ω–æ—Å—Ç—å)
   - –†–µ–ø—Ä–µ–∑–µ–Ω—Ç–∞—Ç–∏–≤–Ω—ã–µ –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ –¥–ª—è –∫–∞–∂–¥–æ–π —Ç–µ–º—ã
   - –ü—Ä–∏–º–µ—Ä—ã —Å—Ç–∞—Ç–µ–π –¥–ª—è –∫–∞–∂–¥–æ–π —Ç–µ–º—ã

2. **–ê–ù–ê–õ–ò–ó –¢–†–ï–ù–î–û–í**
   - –†–∞—Å—Ç—É—â–∏–µ —Ç–µ–º—ã (emerging topics)
   - –£–≥–∞—Å–∞—é—â–∏–µ —Ç–µ–º—ã (declining topics)
   - –°—Ç–∞–±–∏–ª—å–Ω—ã–µ/—É—Å—Ç–æ–π—á–∏–≤—ã–µ —Ç–µ–º—ã
   - –í—Ä–µ–º–µ–Ω–Ω–∞—è —ç–≤–æ–ª—é—Ü–∏—è —Ç–µ–º–∞—Ç–∏–∫

3. **–í–ó–ê–ò–ú–û–°–í–Ø–ó–ò –¢–ï–ú**
   - –ö–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏—è –ø–æ—Ö–æ–∂–∏—Ö —Ç–µ–º
   - –°–≤—è–∑–∏ –º–µ–∂–¥—É —Ä–∞–∑–ª–∏—á–Ω—ã–º–∏ —Ç–µ–º–∞–º–∏
   - –ò–µ—Ä–∞—Ä—Ö–∏—è —Ç–µ–º –∏ –ø–æ–¥—Ç–µ–º
   - –ö–æ—Ä—Ä–µ–ª—è—Ü–∏–∏ –∏ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏

4. **–ü–†–û–ì–ù–û–ó–ù–ê–Ø –ê–ù–ê–õ–ò–¢–ò–ö–ê**
   - –ù–∞–∏–±–æ–ª–µ–µ –ø–µ—Ä—Å–ø–µ–∫—Ç–∏–≤–Ω—ã–µ —Ç–µ–º—ã –¥–ª—è –æ—Ç—Å–ª–µ–∂–∏–≤–∞–Ω–∏—è
   - –í–ª–∏—è–Ω–∏–µ –Ω–∞ —Ä—ã–Ω–æ–∫ –∏ –∏–Ω–¥—É—Å—Ç—Ä–∏—é
   - –ü—Ä–æ–≥–Ω–æ–∑ –ø–æ—è–≤–ª–µ–Ω–∏—è –Ω–æ–≤—ã—Ö —Ç–µ–º
   - –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –ø–æ —Ñ–æ–∫—É—Å–∏—Ä–æ–≤–∫–µ –≤–Ω–∏–º–∞–Ω–∏—è

–ò—Å–ø–æ–ª—å–∑—É–π –≤–∏–∑—É–∞–ª—å–Ω–æ–µ —Ñ–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ —Å —ç–º–æ–¥–∑–∏ –∏ —á–µ—Ç–∫—É—é –∫–∞—Ç–µ–≥–æ—Ä–∏–∑–∞—Ü–∏—é."""

        try:
            topics_result = self.gpt5_service.send_analysis(
                prompt,
                max_output_tokens=1500,
                verbosity="high",
                reasoning_effort="medium"
            )

            # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–π –∞–Ω–∞–ª–∏–∑ —Ç–µ–º
            topic_metrics = self._calculate_topic_metrics(articles)

            return {
                'topics_analysis': topics_result,
                'topic_metrics': topic_metrics,
                'trend_indicators': self._calculate_trend_indicators(articles),
                'topic_evolution': self._analyze_topic_evolution(articles)
            }

        except Exception as e:
            logger.error(f"‚ùå Topics error: {e}")
            return {'error': str(e), 'topics': '–ê–Ω–∞–ª–∏–∑ —Ç–µ–º –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω'}

    # –í—Å–ø–æ–º–æ–≥–∞—Ç–µ–ª—å–Ω—ã–µ –º–µ—Ç–æ–¥—ã

    def _format_articles_for_gpt(self, articles: List[Dict[str, Any]], include_metadata: bool = False) -> str:
        """–§–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ —Å—Ç–∞—Ç–µ–π –¥–ª—è GPT-5"""
        if not articles:
            return "–ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞"

        formatted = []
        for i, article in enumerate(articles):
            title = article.get('title', '–ë–µ–∑ –Ω–∞–∑–≤–∞–Ω–∏—è')[:150]
            content = article.get('content', '')[:400]

            text = f"–°—Ç–∞—Ç—å—è {i+1}:\n–ù–∞–∑–≤–∞–Ω–∏–µ: {title}\n–°–æ–¥–µ—Ä–∂–∞–Ω–∏–µ: {content}\n"

            if include_metadata:
                source = article.get('source', '–ù–µ–∏–∑–≤–µ—Å—Ç–Ω–æ')
                date = article.get('published_at', '–ù–µ–∏–∑–≤–µ—Å—Ç–Ω–æ')
                score = article.get('score', 0)
                text += f"–ò—Å—Ç–æ—á–Ω–∏–∫: {source}\n–î–∞—Ç–∞: {date}\n–†–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç—å: {score:.2f}\n"

            text += "---\n"
            formatted.append(text)

        return '\n'.join(formatted)

    def _calculate_analysis_metrics(self, articles: List[Dict[str, Any]]) -> Dict[str, Any]:
        """–†–∞—Å—á–µ—Ç –º–µ—Ç—Ä–∏–∫ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞"""
        if not articles:
            return {}

        return {
            'avg_relevance': sum(a.get('score', 0) for a in articles) / len(articles),
            'source_diversity': len(set(a.get('source_domain', '') for a in articles)),
            'time_span_days': self._calculate_time_span(articles),
            'content_density': sum(len(a.get('content', '')) for a in articles) / len(articles)
        }

    def _get_date_range(self, articles: List[Dict[str, Any]]) -> Dict[str, str]:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ –≤—Ä–µ–º–µ–Ω–Ω–æ–≥–æ –¥–∏–∞–ø–∞–∑–æ–Ω–∞ —Å—Ç–∞—Ç–µ–π"""
        if not articles:
            return {}

        dates = [a.get('published_at', '') for a in articles if a.get('published_at')]
        if not dates:
            return {}

        return {
            'earliest': min(dates),
            'latest': max(dates)
        }

    def _get_top_sources(self, articles: List[Dict[str, Any]], limit: int = 5) -> List[Dict[str, Any]]:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ —Ç–æ–ø –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤"""
        source_counts = {}
        for article in articles:
            source = article.get('source_domain', 'Unknown')
            source_counts[source] = source_counts.get(source, 0) + 1

        return [
            {'source': source, 'count': count}
            for source, count in sorted(source_counts.items(), key=lambda x: x[1], reverse=True)[:limit]
        ]

    def _get_average_score(self, articles: List[Dict[str, Any]]) -> float:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ —Å—Ä–µ–¥–Ω–µ–π –æ—Ü–µ–Ω–∫–∏ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç–∏"""
        if not articles:
            return 0.0

        scores = [a.get('score', 0) for a in articles]
        return sum(scores) / len(scores) if scores else 0.0

    def _calculate_coverage_stats(self, articles: List[Dict[str, Any]]) -> Dict[str, Any]:
        """–†–∞—Å—á–µ—Ç —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ –ø–æ–∫—Ä—ã—Ç–∏—è"""
        return {
            'sources_count': len(set(a.get('source_domain', '') for a in articles)),
            'avg_content_length': sum(len(a.get('content', '')) for a in articles) / len(articles) if articles else 0,
            'date_coverage': self._get_date_range(articles)
        }

    def _perform_data_aggregation(self, articles: List[Dict[str, Any]], metric: str, groupby: str) -> Dict[str, Any]:
        """–í—ã–ø–æ–ª–Ω–µ–Ω–∏–µ –∞–≥—Ä–µ–≥–∞—Ü–∏–∏ –¥–∞–Ω–Ω—ã—Ö"""
        if not articles:
            return {}

        aggregation = {}

        if groupby == '–∏—Å—Ç–æ—á–Ω–∏–∫':
            for article in articles:
                source = article.get('source_domain', 'Unknown')
                if source not in aggregation:
                    aggregation[source] = {'count': 0, 'scores': []}
                aggregation[source]['count'] += 1
                aggregation[source]['scores'].append(article.get('score', 0))

        elif groupby == '–¥–∞—Ç–∞':
            for article in articles:
                date = article.get('published_at', '')[:10]  # YYYY-MM-DD
                if date not in aggregation:
                    aggregation[date] = {'count': 0, 'scores': []}
                aggregation[date]['count'] += 1
                aggregation[date]['scores'].append(article.get('score', 0))

        return aggregation

    def _apply_smart_filters(self, articles: List[Dict[str, Any]], criteria: str, value: str) -> List[Dict[str, Any]]:
        """–ü—Ä–∏–º–µ–Ω–µ–Ω–∏–µ —É–º–Ω—ã—Ö —Ñ–∏–ª—å—Ç—Ä–æ–≤"""
        filtered = []

        for article in articles:
            match = False

            if criteria == '–∏—Å—Ç–æ—á–Ω–∏–∫':
                if value.lower() in article.get('source_domain', '').lower():
                    match = True
            elif criteria == '–¥–∞—Ç–∞':
                if value in article.get('published_at', ''):
                    match = True
            elif criteria == '—Å–æ–¥–µ—Ä–∂–∞–Ω–∏–µ':
                if value.lower() in article.get('content', '').lower():
                    match = True
            elif criteria == '–Ω–∞–∑–≤–∞–Ω–∏–µ':
                if value.lower() in article.get('title', '').lower():
                    match = True

            if match:
                filtered.append(article)

        return filtered

    def _calculate_business_metrics(self, articles: List[Dict[str, Any]]) -> Dict[str, Any]:
        """–†–∞—Å—á–µ—Ç –±–∏–∑–Ω–µ—Å-–º–µ—Ç—Ä–∏–∫"""
        return {
            'market_coverage': len(set(a.get('source_domain', '') for a in articles)),
            'information_velocity': len(articles) / max(1, self._calculate_time_span(articles)),
            'content_quality_score': self._get_average_score(articles),
            'trend_strength': self._calculate_trend_strength(articles)
        }

    def _calculate_confidence_score(self, articles: List[Dict[str, Any]]) -> float:
        """–†–∞—Å—á–µ—Ç —É—Ä–æ–≤–Ω—è —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç–∏ –≤ –∞–Ω–∞–ª–∏–∑–µ"""
        if not articles:
            return 0.0

        # –§–∞–∫—Ç–æ—Ä—ã —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç–∏: –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å—Ç–∞—Ç–µ–π, —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–∏–µ –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤, –∫–∞—á–µ—Å—Ç–≤–æ –∫–æ–Ω—Ç–µ–Ω—Ç–∞
        article_factor = min(1.0, len(articles) / 20)  # –¥–æ 20 —Å—Ç–∞—Ç–µ–π = –º–∞–∫—Å–∏–º—É–º
        source_factor = min(1.0, len(set(a.get('source_domain', '') for a in articles)) / 5)  # –¥–æ 5 –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤
        quality_factor = self._get_average_score(articles)

        return (article_factor + source_factor + quality_factor) / 3

    def _extract_market_indicators(self, articles: List[Dict[str, Any]]) -> Dict[str, Any]:
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Ä—ã–Ω–æ—á–Ω—ã—Ö –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä–æ–≤"""
        return {
            'publication_frequency': len(articles),
            'source_diversity': len(set(a.get('source_domain', '') for a in articles)),
            'average_relevance': self._get_average_score(articles),
            'time_distribution': self._analyze_time_distribution(articles)
        }

    def _calculate_sentiment_metrics(self, articles: List[Dict[str, Any]]) -> Dict[str, Any]:
        """–ë–∞–∑–æ–≤—ã–π —Ä–∞—Å—á–µ—Ç –º–µ—Ç—Ä–∏–∫ —Ç–æ–Ω–∞–ª—å–Ω–æ—Å—Ç–∏"""
        # –ü—Ä–∏–º–µ—Ä–Ω—ã–π –∞–Ω–∞–ª–∏–∑ –Ω–∞ –æ—Å–Ω–æ–≤–µ –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤
        positive_words = ['—Ä–æ—Å—Ç', '—É—Å–ø–µ—Ö', '–ø—Ä–æ–≥—Ä–µ—Å—Å', '—Ä–∞–∑–≤–∏—Ç–∏–µ', '—É–ª—É—á—à–µ–Ω–∏–µ', '–∏–Ω–Ω–æ–≤–∞—Ü–∏–∏']
        negative_words = ['—Å–ø–∞–¥', '–ø—Ä–æ–±–ª–µ–º–∞', '–∫—Ä–∏–∑–∏—Å', '—Å–Ω–∏–∂–µ–Ω–∏–µ', '—Ä–∏—Å–∫', '—É–≥—Ä–æ–∑–∞']

        sentiment_scores = []
        for article in articles:
            content = (article.get('title', '') + ' ' + article.get('content', '')).lower()
            positive_count = sum(1 for word in positive_words if word in content)
            negative_count = sum(1 for word in negative_words if word in content)

            if positive_count + negative_count > 0:
                score = (positive_count - negative_count) / (positive_count + negative_count)
            else:
                score = 0
            sentiment_scores.append(score)

        avg_sentiment = sum(sentiment_scores) / len(sentiment_scores) if sentiment_scores else 0

        return {
            'average_sentiment': avg_sentiment,
            'positive_ratio': sum(1 for s in sentiment_scores if s > 0.1) / len(sentiment_scores) if sentiment_scores else 0,
            'negative_ratio': sum(1 for s in sentiment_scores if s < -0.1) / len(sentiment_scores) if sentiment_scores else 0,
            'neutral_ratio': sum(1 for s in sentiment_scores if -0.1 <= s <= 0.1) / len(sentiment_scores) if sentiment_scores else 0
        }

    def _extract_emotional_indicators(self, articles: List[Dict[str, Any]]) -> Dict[str, Any]:
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —ç–º–æ—Ü–∏–æ–Ω–∞–ª—å–Ω—ã—Ö –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä–æ–≤"""
        return {
            'urgency_level': self._calculate_urgency_level(articles),
            'optimism_score': self._calculate_optimism_score(articles),
            'concern_level': self._calculate_concern_level(articles)
        }

    def _calculate_sentiment_confidence(self, articles: List[Dict[str, Any]]) -> float:
        """–†–∞—Å—á–µ—Ç —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç–∏ –≤ –∞–Ω–∞–ª–∏–∑–µ —Ç–æ–Ω–∞–ª—å–Ω–æ—Å—Ç–∏"""
        return min(1.0, len(articles) / 10)  # –£–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å —Ä–∞—Å—Ç–µ—Ç —Å –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ–º —Å—Ç–∞—Ç–µ–π

    def _calculate_topic_metrics(self, articles: List[Dict[str, Any]]) -> Dict[str, Any]:
        """–†–∞—Å—á–µ—Ç –º–µ—Ç—Ä–∏–∫ –ø–æ —Ç–µ–º–∞–º"""
        return {
            'topic_diversity': self._calculate_topic_diversity(articles),
            'keyword_density': self._calculate_keyword_density(articles),
            'thematic_coherence': self._calculate_thematic_coherence(articles)
        }

    def _calculate_trend_indicators(self, articles: List[Dict[str, Any]]) -> Dict[str, Any]:
        """–†–∞—Å—á–µ—Ç –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä–æ–≤ —Ç—Ä–µ–Ω–¥–æ–≤"""
        return {
            'publication_trend': self._analyze_publication_trend(articles),
            'interest_momentum': self._calculate_interest_momentum(articles),
            'topic_evolution_rate': self._calculate_topic_evolution_rate(articles)
        }

    def _analyze_topic_evolution(self, articles: List[Dict[str, Any]]) -> Dict[str, Any]:
        """–ê–Ω–∞–ª–∏–∑ —ç–≤–æ–ª—é—Ü–∏–∏ —Ç–µ–º"""
        return {
            'emerging_themes': self._identify_emerging_themes(articles),
            'declining_themes': self._identify_declining_themes(articles),
            'stable_themes': self._identify_stable_themes(articles)
        }

    # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –≤—Å–ø–æ–º–æ–≥–∞—Ç–µ–ª—å–Ω—ã–µ –º–µ—Ç–æ–¥—ã –¥–ª—è —Ä–∞—Å—á–µ—Ç–æ–≤

    def _calculate_time_span(self, articles: List[Dict[str, Any]]) -> int:
        """–†–∞—Å—á–µ—Ç –≤—Ä–µ–º–µ–Ω–Ω–æ–≥–æ –ø—Ä–æ–º–µ–∂—É—Ç–∫–∞ –≤ –¥–Ω—è—Ö"""
        dates = [a.get('published_at', '') for a in articles if a.get('published_at')]
        if not dates:
            return 1

        try:
            date_objects = [datetime.fromisoformat(d.replace('Z', '+00:00')) for d in dates]
            span = (max(date_objects) - min(date_objects)).days
            return max(1, span)
        except:
            return 1

    def _calculate_trend_strength(self, articles: List[Dict[str, Any]]) -> float:
        """–†–∞—Å—á–µ—Ç —Å–∏–ª—ã —Ç—Ä–µ–Ω–¥–∞"""
        return min(1.0, len(articles) / 50)  # –£—Å–ª–æ–≤–Ω–∞—è —Ñ–æ—Ä–º—É–ª–∞

    def _analyze_time_distribution(self, articles: List[Dict[str, Any]]) -> Dict[str, int]:
        """–ê–Ω–∞–ª–∏–∑ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –ø–æ –≤—Ä–µ–º–µ–Ω–∏"""
        distribution = {}
        for article in articles:
            date = article.get('published_at', '')[:10]
            distribution[date] = distribution.get(date, 0) + 1
        return distribution

    def _calculate_urgency_level(self, articles: List[Dict[str, Any]]) -> float:
        """–†–∞—Å—á–µ—Ç —É—Ä–æ–≤–Ω—è —Å—Ä–æ—á–Ω–æ—Å—Ç–∏"""
        urgent_words = ['—Å—Ä–æ—á–Ω–æ', '–Ω–µ–º–µ–¥–ª–µ–Ω–Ω–æ', '–∫—Ä–∏—Ç–∏—á–Ω–æ', '—ç–∫—Å—Ç—Ä–µ–Ω–Ω–æ']
        total_urgency = 0
        for article in articles:
            content = (article.get('title', '') + ' ' + article.get('content', '')).lower()
            total_urgency += sum(1 for word in urgent_words if word in content)
        return total_urgency / len(articles) if articles else 0

    def _calculate_optimism_score(self, articles: List[Dict[str, Any]]) -> float:
        """–†–∞—Å—á–µ—Ç —É—Ä–æ–≤–Ω—è –æ–ø—Ç–∏–º–∏–∑–º–∞"""
        optimistic_words = ['–Ω–∞–¥–µ–∂–¥–∞', '–ø–µ—Ä—Å–ø–µ–∫—Ç–∏–≤–∞', '–≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç—å', '–ø–æ—Ç–µ–Ω—Ü–∏–∞–ª']
        total_optimism = 0
        for article in articles:
            content = (article.get('title', '') + ' ' + article.get('content', '')).lower()
            total_optimism += sum(1 for word in optimistic_words if word in content)
        return total_optimism / len(articles) if articles else 0

    def _calculate_concern_level(self, articles: List[Dict[str, Any]]) -> float:
        """–†–∞—Å—á–µ—Ç —É—Ä–æ–≤–Ω—è –æ–±–µ—Å–ø–æ–∫–æ–µ–Ω–Ω–æ—Å—Ç–∏"""
        concern_words = ['–±–µ—Å–ø–æ–∫–æ–π—Å—Ç–≤–æ', '—Ç—Ä–µ–≤–æ–≥–∞', '–æ–∑–∞–±–æ—á–µ–Ω–Ω–æ—Å—Ç—å', '–æ–ø–∞—Å–µ–Ω–∏–µ']
        total_concern = 0
        for article in articles:
            content = (article.get('title', '') + ' ' + article.get('content', '')).lower()
            total_concern += sum(1 for word in concern_words if word in content)
        return total_concern / len(articles) if articles else 0

    def _calculate_topic_diversity(self, articles: List[Dict[str, Any]]) -> float:
        """–†–∞—Å—á–µ—Ç —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–∏—è —Ç–µ–º"""
        all_words = []
        for article in articles:
            content = (article.get('title', '') + ' ' + article.get('content', '')).lower()
            words = content.split()
            all_words.extend(words)

        unique_words = len(set(all_words))
        total_words = len(all_words)
        return unique_words / total_words if total_words > 0 else 0

    def _calculate_keyword_density(self, articles: List[Dict[str, Any]]) -> float:
        """–†–∞—Å—á–µ—Ç –ø–ª–æ—Ç–Ω–æ—Å—Ç–∏ –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤"""
        # –£–ø—Ä–æ—â–µ–Ω–Ω—ã–π —Ä–∞—Å—á–µ—Ç
        return len(articles) * 0.1  # –£—Å–ª–æ–≤–Ω–∞—è —Ñ–æ—Ä–º—É–ª–∞

    def _calculate_thematic_coherence(self, articles: List[Dict[str, Any]]) -> float:
        """–†–∞—Å—á–µ—Ç —Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–æ–π —Å–æ–≥–ª–∞—Å–æ–≤–∞–Ω–Ω–æ—Å—Ç–∏"""
        return min(1.0, len(articles) / 30)  # –£—Å–ª–æ–≤–Ω–∞—è —Ñ–æ—Ä–º—É–ª–∞

    def _analyze_publication_trend(self, articles: List[Dict[str, Any]]) -> str:
        """–ê–Ω–∞–ª–∏–∑ —Ç—Ä–µ–Ω–¥–∞ –ø—É–±–ª–∏–∫–∞—Ü–∏–π"""
        if len(articles) > 10:
            return "—Ä–∞—Å—Ç—É—â–∏–π"
        elif len(articles) > 5:
            return "—Å—Ç–∞–±–∏–ª—å–Ω—ã–π"
        else:
            return "—Å–Ω–∏–∂–∞—é—â–∏–π—Å—è"

    def _calculate_interest_momentum(self, articles: List[Dict[str, Any]]) -> float:
        """–†–∞—Å—á–µ—Ç –º–æ–º–µ–Ω—Ç—É–º–∞ –∏–Ω—Ç–µ—Ä–µ—Å–∞"""
        return len(articles) / 10  # –£—Å–ª–æ–≤–Ω–∞—è —Ñ–æ—Ä–º—É–ª–∞

    def _calculate_topic_evolution_rate(self, articles: List[Dict[str, Any]]) -> float:
        """–†–∞—Å—á–µ—Ç —Å–∫–æ—Ä–æ—Å—Ç–∏ —ç–≤–æ–ª—é—Ü–∏–∏ —Ç–µ–º"""
        return len(set(a.get('source_domain', '') for a in articles)) / 5  # –£—Å–ª–æ–≤–Ω–∞—è —Ñ–æ—Ä–º—É–ª–∞

    def _identify_emerging_themes(self, articles: List[Dict[str, Any]]) -> List[str]:
        """–í—ã—è–≤–ª–µ–Ω–∏–µ —Ä–∞–∑–≤–∏–≤–∞—é—â–∏—Ö—Å—è —Ç–µ–º"""
        return ["–Ω–æ–≤—ã–µ —Ç–µ—Ö–Ω–æ–ª–æ–≥–∏–∏", "—Ü–∏—Ñ—Ä–æ–≤–∞—è —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–∞—Ü–∏—è"]  # –ó–∞–≥–ª—É—à–∫–∞

    def _identify_declining_themes(self, articles: List[Dict[str, Any]]) -> List[str]:
        """–í—ã—è–≤–ª–µ–Ω–∏–µ —É–≥–∞—Å–∞—é—â–∏—Ö —Ç–µ–º"""
        return ["—É—Å—Ç–∞—Ä–µ–≤—à–∏–µ –ø–æ–¥—Ö–æ–¥—ã"]  # –ó–∞–≥–ª—É—à–∫–∞

    def _identify_stable_themes(self, articles: List[Dict[str, Any]]) -> List[str]:
        """–í—ã—è–≤–ª–µ–Ω–∏–µ —Å—Ç–∞–±–∏–ª—å–Ω—ã—Ö —Ç–µ–º"""
        return ["–æ—Å–Ω–æ–≤–Ω—ã–µ —Ç—Ä–µ–Ω–¥—ã", "—Ä—ã–Ω–æ—á–Ω—ã–µ –∏–∑–º–µ–Ω–µ–Ω–∏—è"]  # –ó–∞–≥–ª—É—à–∫–∞


# –ì–ª–∞–≤–Ω—ã–µ —Ñ—É–Ω–∫—Ü–∏–∏ –¥–ª—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è

async def analyze_data(query: str, timeframe: str = "7d", **kwargs) -> AnalysisResult:
    """–ì–ª—É–±–æ–∫–∏–π –∞–Ω–∞–ª–∏–∑ –¥–∞–Ω–Ω—ã—Ö"""
    processor = DataAnalysisProcessor()
    request = AnalysisRequest(
        command=AnalysisType.DEEP_ANALYSIS,
        query=query,
        timeframe=timeframe,
        parameters=kwargs
    )
    return await processor.process_analysis_request(request)

async def summarize_data(topic: str, length: str = "medium", timeframe: str = "7d") -> AnalysisResult:
    """AI-powered —Å—É–º–º–∞—Ä–∏–∑–∞—Ü–∏—è"""
    processor = DataAnalysisProcessor()
    request = AnalysisRequest(
        command=AnalysisType.SUMMARY,
        query=topic,
        timeframe=timeframe,
        parameters={'length': length}
    )
    return await processor.process_analysis_request(request)

async def aggregate_data(metric: str, groupby: str, query: str = "", timeframe: str = "7d") -> AnalysisResult:
    """–ê–≥—Ä–µ–≥–∞—Ü–∏—è –¥–∞–Ω–Ω—ã—Ö"""
    processor = DataAnalysisProcessor()
    request = AnalysisRequest(
        command=AnalysisType.AGGREGATION,
        query=query or "–æ–±—â–∏–π –∞–Ω–∞–ª–∏–∑",
        timeframe=timeframe,
        parameters={'metric': metric, 'groupby': groupby}
    )
    return await processor.process_analysis_request(request)

async def filter_data(criteria: str, value: str, query: str = "", timeframe: str = "7d") -> AnalysisResult:
    """–£–º–Ω–∞—è —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏—è"""
    processor = DataAnalysisProcessor()
    request = AnalysisRequest(
        command=AnalysisType.FILTERING,
        query=query or "—Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏—è –¥–∞–Ω–Ω—ã—Ö",
        timeframe=timeframe,
        parameters={'criteria': criteria, 'value': value}
    )
    return await processor.process_analysis_request(request)

async def generate_insights(topic: str, timeframe: str = "7d") -> AnalysisResult:
    """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –±–∏–∑–Ω–µ—Å-–∏–Ω—Å–∞–π—Ç–æ–≤"""
    processor = DataAnalysisProcessor()
    request = AnalysisRequest(
        command=AnalysisType.INSIGHTS,
        query=topic,
        timeframe=timeframe
    )
    return await processor.process_analysis_request(request)

async def analyze_sentiment(query: str, timeframe: str = "7d") -> AnalysisResult:
    """–ê–Ω–∞–ª–∏–∑ —Ç–æ–Ω–∞–ª—å–Ω–æ—Å—Ç–∏"""
    processor = DataAnalysisProcessor()
    request = AnalysisRequest(
        command=AnalysisType.SENTIMENT,
        query=query,
        timeframe=timeframe
    )
    return await processor.process_analysis_request(request)

async def analyze_topics(scope: str, timeframe: str = "7d") -> AnalysisResult:
    """–ú–æ–¥–µ–ª–∏—Ä–æ–≤–∞–Ω–∏–µ —Ç–µ–º –∏ –∞–Ω–∞–ª–∏–∑ —Ç—Ä–µ–Ω–¥–æ–≤"""
    processor = DataAnalysisProcessor()
    request = AnalysisRequest(
        command=AnalysisType.TOPICS,
        query=scope,
        timeframe=timeframe,
        parameters={'scope': scope}
    )
    return await processor.process_analysis_request(request)


# CLI –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å –¥–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è
if __name__ == "__main__":
    import argparse

    parser = argparse.ArgumentParser(description="GPT-5 Data Analysis Processor")
    parser.add_argument("command", choices=["analyze", "summarize", "aggregate", "filter", "insights", "sentiment", "topics"])
    parser.add_argument("query", help="Search query or topic")
    parser.add_argument("--timeframe", default="7d", help="Time frame (e.g., 7d, 2w, 1m)")
    parser.add_argument("--param1", help="First parameter (e.g., length for summarize, metric for aggregate)")
    parser.add_argument("--param2", help="Second parameter (e.g., groupby for aggregate)")

    args = parser.parse_args()

    async def main():
        try:
            if args.command == "analyze":
                result = await analyze_data(args.query, args.timeframe)
            elif args.command == "summarize":
                result = await summarize_data(args.query, args.param1 or "medium", args.timeframe)
            elif args.command == "aggregate":
                result = await aggregate_data(args.param1 or "–∏—Å—Ç–æ—á–Ω–∏–∫–∏", args.param2 or "–¥–∞—Ç–∞", args.query, args.timeframe)
            elif args.command == "filter":
                result = await filter_data(args.param1 or "–∏—Å—Ç–æ—á–Ω–∏–∫", args.param2 or "tech.com", args.query, args.timeframe)
            elif args.command == "insights":
                result = await generate_insights(args.query, args.timeframe)
            elif args.command == "sentiment":
                result = await analyze_sentiment(args.query, args.timeframe)
            elif args.command == "topics":
                result = await analyze_topics(args.query, args.timeframe)

            print(f"\nü§ñ –†–µ–∑—É–ª—å—Ç–∞—Ç –∞–Ω–∞–ª–∏–∑–∞ '{args.command}':")
            print(f"‚è±Ô∏è –í—Ä–µ–º—è –æ–±—Ä–∞–±–æ—Ç–∫–∏: {result.processing_time:.2f}—Å")
            print(f"‚úÖ –£—Å–ø–µ—à–Ω–æ: {result.success}")

            if result.success:
                print(f"üìä –ú–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ: {json.dumps(result.metadata, ensure_ascii=False, indent=2)}")
                print(f"üìÑ –û—Å–Ω–æ–≤–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ:")
                for key, value in result.data.items():
                    if isinstance(value, str) and len(value) > 200:
                        print(f"  {key}: {value[:200]}...")
                    else:
                        print(f"  {key}: {value}")
            else:
                print(f"‚ùå –û—à–∏–±–∫–∞: {result.error}")

        except Exception as e:
            print(f"‚ùå –ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞: {e}")
            import traceback
            print(traceback.format_exc())

    asyncio.run(main())
